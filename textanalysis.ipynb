{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curious Comments \n",
    "![commentpic](comment_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical part of any review are the comments. We will now proceed to analyze the comments in our q data. We will judge the predictive power of these comments, and analyze the role they play in a score's q rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import os\n",
    "import math\n",
    "from itertools import chain\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating a dataframe of comments. We will randomly subsample 8 comments per review and only consider courses that have at least 10 comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_PER_COURSE_SAMPLES = 8\n",
    "#print tempdf[tempdf['C_Number'] == 'SOC-STD 98nw'].Comments.tolist()[0]\n",
    "#tempdf.groupby('C_Number').groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot compare a dtyped [object] array with a scalar of type [bool]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-c42957210aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcourseNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtempdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'C_Department'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C_Number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mallCourseComments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mallSemesterComments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtempdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_Number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcourseNum\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtempdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_Department\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdept\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meachSemesterComments\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallSemesterComments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mallCourseComments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallCourseComments\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meachSemesterComments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angelama/anaconda/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# scalars, list, tuple, np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mfiller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_int\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_self_int_dtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfill_bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m             return filler(self._constructor(na_op(self.values, other),\n\u001b[0m\u001b[1;32m    797\u001b[0m                                     index=self.index)).__finalize__(self)\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/angelama/anaconda/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    767\u001b[0m                     raise TypeError(\"cannot compare a dtyped [{0}] array with \"\n\u001b[1;32m    768\u001b[0m                                     \"a scalar of type [{1}]\".format(\n\u001b[0;32m--> 769\u001b[0;31m                                         x.dtype, type(y).__name__))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot compare a dtyped [object] array with a scalar of type [bool]"
     ]
    }
   ],
   "source": [
    "bigdf=pd.read_csv(\"bigdf.csv\")\n",
    "bigdf.reset_index(drop=True)\n",
    "tempdf = bigdf[['C_Department','C_Number','Positive','Comments']].dropna()\n",
    "subdf = pd.DataFrame()\n",
    "for dept, courseNum in tempdf.groupby(['C_Department','C_Number']).groups:\n",
    "    allCourseComments = []\n",
    "    allSemesterComments = tempdf[tempdf.C_Number == courseNum & tempdf.C_Department == dept].Comments.tolist()\n",
    "    for eachSemesterComments in allSemesterComments:\n",
    "        allCourseComments = allCourseComments + ast.literal_eval(eachSemesterComments)\n",
    "    if len(allCourseComments) >= 10:\n",
    "        sample = np.random.choice(allCourseComments, MAX_PER_COURSE_SAMPLES, replace=False)\n",
    "        coursedf = pd.DataFrame()\n",
    "        coursedf['Comment'] = sample\n",
    "        coursedf['Course'] = dept + courseNum\n",
    "        subdf = pd.concat([subdf,coursedf]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our comments dataframe, subdf, to a spark dataframe for text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec\n"
     ]
    }
   ],
   "source": [
    "#setup spark\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"5g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()\n",
    "sys.version\n",
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS\n",
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a get parts function to parse the language in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|             Comment|    C_Number|\n",
      "+--------------------+------------+\n",
      "|Take this class i...|SOC-STD 98nw|\n",
      "|Be prepared to le...|SOC-STD 98nw|\n",
      "|The course is cha...|SOC-STD 98nw|\n",
      "|This is a great c...|SOC-STD 98nw|\n",
      "|If you are at all...|SOC-STD 98nw|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subdf = sqlsc.createDataFrame(subdf)\n",
    "subdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[u'class', u'thesis', u'field', u'work'],\n",
       "   [u'course', u'experience', u'research', u'thesis'],\n",
       "   [u'healthcare', u'way', u'problem', u'day'],\n",
       "   [u'note', u'tea', u'party', u'elephant', u'bias', u'course'],\n",
       "   [u'person', u'view', u'coverage'],\n",
       "   [u'view']],\n",
       "  [[u'actual'],\n",
       "   [u'true', u'original', u'qualitative'],\n",
       "   [u'great', u'important', u'political', u'sociological', u'economic'],\n",
       "   [u'super', u'conservative', u'obsessed', u'general', u'liberal'],\n",
       "   [u'moderate', u'political', u'universal', u'important'],\n",
       "   [u'better']]),\n",
       " ([[u'healthcare', u'individual'], [u'class', u'reading', u'time']],\n",
       "  [[u'prepared', u'knowledgeable'], [u'great', u'little', u'unstructured']]),\n",
       " ([[u'course', u'workload', u'load'],\n",
       "   [u'course'],\n",
       "   [u'practice', u'research', u'thesis']],\n",
       "  [[u'great'], [u'glad'], [u'confident', u'well-equipped', u'senior']]),\n",
       " ([[u'class', u'writing', u'thesis', u'topic', u'healthcare', u'healthcare'],\n",
       "   [u'course', u'material'],\n",
       "   [u'method', u'thesis', u'option']],\n",
       "  [[u'great', u'interested'], [u'enjoyable'], [u'great']]),\n",
       " ([[u'healthcare', u'class'],\n",
       "   [u'history', u'class', u'knowledge', u'history', u'healthcare', u'class'],\n",
       "   [u'appreciation', u'understanding', u'way', u'healthcare', u'today'],\n",
       "   [u'healthcare', u'citizen', u'healthcare', u'policy', u'class']],\n",
       "  [[u'interested', u'american'],\n",
       "   [u'huge'],\n",
       "   [u'greater'],\n",
       "   [u'interested', u'informed', u'current']])]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_parts = subdf.rdd.map(lambda r: get_parts(r.Comment))\n",
    "comment_parts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 152 ms, total: 324 ms\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsedcomments=comment_parts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin our text analysis with an LDA of the nouns in the comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'class', u'thesis', u'field', u'work'],\n",
       "  [u'course', u'experience', u'research', u'thesis'],\n",
       "  [u'healthcare', u'way', u'problem', u'day'],\n",
       "  [u'note', u'tea', u'party', u'elephant', u'bias', u'course'],\n",
       "  [u'person', u'view', u'coverage'],\n",
       "  [u'view']],\n",
       " [[u'healthcare', u'individual'], [u'class', u'reading', u'time']],\n",
       " [[u'course', u'workload', u'load'],\n",
       "  [u'course'],\n",
       "  [u'practice', u'research', u'thesis']]]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e[0] for e in parsedcomments[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'class', u'thesis', u'field', u'work'],\n",
       " [u'course', u'experience', u'research', u'thesis'],\n",
       " [u'healthcare', u'way', u'problem', u'day'],\n",
       " [u'note', u'tea', u'party', u'elephant', u'bias', u'course'],\n",
       " [u'person', u'view', u'coverage']]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldadatardd=sc.parallelize([ele[0] for ele in parsedcomments]).flatMap(lambda l: l)\n",
    "ldadatardd.cache()\n",
    "ldadatardd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'class', u'thesis', u'field', u'work', u'course']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldadatardd.flatMap(lambda word: word).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabtups = (ldadatardd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab=vocabtups.collectAsMap()\n",
    "id2word=vocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'req', u'dynamic', 5)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0], vocab.keys()[5], vocab[vocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4661"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def helperfunction(element):\n",
    "    d = defaultdict(int)\n",
    "    for k in element:\n",
    "        d[vocab[k]] += 1\n",
    "    return d.items()\n",
    "documents = ldadatardd.map(lambda w: helperfunction(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(3660, 1), (4627, 1), (292, 1), (195, 1)],\n",
       " [(292, 1), (2404, 1), (1421, 1), (2645, 1)],\n",
       " [(105, 1), (1002, 1), (3788, 1), (4170, 1)],\n",
       " [(3777, 1), (2346, 1), (1421, 1), (4305, 1), (1656, 1), (4216, 1)],\n",
       " [(2242, 1), (1558, 1), (3110, 1)]]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=documents.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda2 = gensim.models.ldamodel.LdaModel(corpus = corpus, num_topics = 2, id2word=id2word, chunksize=200, passes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we print the topics we find using LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.049*course + 0.040*lot + 0.039*material + 0.033*time + 0.026*way + 0.025*work + 0.017*experience + 0.015*paper + 0.014*field + 0.014*person',\n",
       " u'0.171*class + 0.079*course + 0.027*student + 0.026*professor + 0.026*lecture + 0.025*reading + 0.024*history + 0.022*topic + 0.021*discussion + 0.016*fun']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first topic (let us call this Topic 0) includes the combination of words: course, lot, material, time, way, work, experience, paper, field, and person. \n",
    "\n",
    "\n",
    "The second topic (let us call this Topic 1) includes the combination of words: class, course, student, professor, lecture, reading, history, topic, discussion, and fun.\n",
    "\n",
    "\n",
    "Topic 1 seems to encompass the more interactive, qualitative, personable aspects of the course with key terms including student, professor, discussion, lecture, and fun. Topic 0, by contrast, seems to encompass the more solitary, logistical, factual aspects of the course with key terms including material, time, work, experience, paper, and field. One thing worth noticing is that both topics include \"course\" as a key term but only Topic 1 includes \"class\". While \"course\" and \"class\" are often used interchangably in language, \"class\" arguably connotes a more personal, interactive experience than \"course\", which is more administrative and logistical and more likely to be used as an umbrella term for everything from everyday class to homework.\n",
    "\n",
    "\n",
    "In order to further evaluate our intial hypothesis that course reviews are split along two topics (interactive, qualitiative, personable aspects v. solitary, logistical aspects), we will output the words of some sentences, along with the probability of the sentence belonging to Topic 0 and Topic 1, to qualitatively check that our topics are reasonable and supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3660, 1), (4627, 1), (292, 1), (195, 1)]\n",
      "[(0, 0.69985952214752012), (1, 0.30014047785247994)]\n",
      "field class thesis work\n",
      "==========================================\n",
      "[(3425, 1), (1421, 1), (4111, 1), (4627, 1), (2477, 1), (4319, 1), (1567, 1)]\n",
      "[(0, 0.33855889890284857), (1, 0.66144110109715148)]\n",
      "depth course grad class history student question\n",
      "==========================================\n",
      "[(3178, 1), (195, 1), (254, 1)]\n",
      "[(0, 0.62540422226333714), (1, 0.37459577773666292)]\n",
      "bit work lab\n",
      "==========================================\n",
      "[(1113, 1), (4515, 1), (405, 1), (729, 1)]\n",
      "[(0, 0.29945806159612509), (1, 0.70054193840387502)]\n",
      "concept commitment moment life\n",
      "==========================================\n",
      "[(3846, 1), (3110, 1)]\n",
      "[(0, 0.49992283880028238), (1, 0.50007716119971768)]\n",
      "lecture person\n",
      "==========================================\n",
      "[(2370, 1), (3846, 2), (3884, 1), (1805, 1), (792, 1), (1113, 1)]\n",
      "[(0, 0.19442318510549644), (1, 0.8055768148945035)]\n",
      "section lecture exam reading drawback concept\n",
      "==========================================\n",
      "[(4627, 1), (551, 1)]\n",
      "[(0, 0.16684756233003178), (1, 0.83315243766996827)]\n",
      "class entertaining\n",
      "==========================================\n",
      "[(280, 1), (1421, 1), (1551, 1)]\n",
      "[(0, 0.84685027552522507), (1, 0.15314972447477493)]\n",
      "order course thing\n",
      "==========================================\n",
      "[(1465, 1), (538, 1), (1391, 1), (2889, 1), (1343, 1)]\n",
      "[(0, 0.41588311084996671), (1, 0.58411688915003324)]\n",
      "resource professor man kind topic\n",
      "==========================================\n",
      "[(4096, 1), (4627, 1), (694, 1), (2191, 1)]\n",
      "[(0, 0.10593678726928428), (1, 0.89406321273071576)]\n",
      "evolution class pre cinema\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "for bow in corpus[0:1000:100]:\n",
    "    print bow\n",
    "    print lda2.get_document_topics(bow)\n",
    "    print \" \".join([id2word[e[0]] for e in bow])\n",
    "    print \"==========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"sentences\" (or bag-of-words) which have a much greater probability of belonging to Topic 0 include:\n",
    "- field class thesis work\n",
    "- bit work lab\n",
    "- order course thing\n",
    "\n",
    "\n",
    "The words in these sentences relate to more impersonal, logistical aspects of a course. Specifically, \"field\", \"thesis\", \"work\", and \"order\" describe inflexible, solitary, requirement aspects of a course while \"bit\" and \"thing\" are more vague but nevertheless imply a degree of impartiability and detatchedness.\n",
    "\n",
    "\n",
    "\n",
    "The \"setences\" which have a much greater probability of belonging to Topic 1 include:\n",
    "- depth course grad class history student question\n",
    "- concept commitment moment life\n",
    "- section lecture exam reading drawback concept\n",
    "- class entertaining\n",
    "- evolution class pre cinema\n",
    "\n",
    "Words in these sentences that are not present in the previous cluster of setences and that stand out as implying more creative, interactive, person-to-person aspects of a course include \"depth\", \"history\", \"question\", \"concept\", \"commitment\", \"moment\", \"life\", \"drawback\", \"lecture\", and \"entertaining\". \n",
    "\n",
    "The \"sentences\" which have more equal probabilities of belonging to Topic 0 or 1 include:\n",
    "- lecture person\n",
    "- resource professor man kind topic\n",
    "\n",
    "We can observe words implying more logistical aspects (\"resource\", \"topic\") and more interactive, creative aspects (\"professor\", \"kind\"). \n",
    "\n",
    "\n",
    "Of course, words such as \"lecture\" can belong to either topic since lecture is both a logistical, required part of most courses and an engaging, potentially interactive opportunity for students to learn from professors. From our analysis of the topic probabilities and bag-of-words above, however, there appears to be evidence to support our initial hypothesis that course reviews are split along two topics: Topic 0, which includes more solitary, logistical aspects and Topic 1, which includes more interactive, qualitative, personable aspects of a course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DOs:\n",
    "- We can consider doing \"verbs\" (use TextBlob)\n",
    "- Detect \"not\" before adjectives, this shouldn't be too difficult\n",
    "- Text before pushing\n",
    "- Look at differences across departments (Jesse/Andrew)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
