{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curious Comments \n",
    "![commentpic](comment_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical part of any review are the comments. We will now proceed to analyze the comments in our q data. We will judge the predictive power of these comments, and analyze the role they play in a score's q rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import os\n",
    "import math\n",
    "from itertools import chain\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating a dataframe of comments. We will randomly subsample 10 comments per review and only consider courses 10 or more comments. Our dataframe will consist of three columns; Comment, Course and Overall Positive Rating. A course defined to have an 'Overall Positive Rating' if it has been been given more positive ratings than negative ratings across all semesters that it has been rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_PER_COURSESEM_REVIEWS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_Department</th>\n",
       "      <th>C_Number</th>\n",
       "      <th>Course</th>\n",
       "      <th>C_CatNum</th>\n",
       "      <th>C_ID</th>\n",
       "      <th>C_Semester</th>\n",
       "      <th>C_Year</th>\n",
       "      <th>C_Term</th>\n",
       "      <th>C_Overall</th>\n",
       "      <th>C_Workload</th>\n",
       "      <th>C_Difficulty</th>\n",
       "      <th>C_Recommendation</th>\n",
       "      <th>C_Enrollment</th>\n",
       "      <th>C_ResponseRate</th>\n",
       "      <th>I_First</th>\n",
       "      <th>I_Last</th>\n",
       "      <th>I_ID</th>\n",
       "      <th>I_Overall</th>\n",
       "      <th>I_EffectiveLectures</th>\n",
       "      <th>I_Accessible</th>\n",
       "      <th>I_GeneratesEnthusiasm</th>\n",
       "      <th>I_EncouragesParticipation</th>\n",
       "      <th>I_UsefulFeedback</th>\n",
       "      <th>I_ReturnsAssignmentsTimely</th>\n",
       "      <th>QOverall_1</th>\n",
       "      <th>QOverall_2</th>\n",
       "      <th>QOverall_3</th>\n",
       "      <th>QOverall_4</th>\n",
       "      <th>QOverall_5</th>\n",
       "      <th>QDifficulty_1</th>\n",
       "      <th>QDifficulty_2</th>\n",
       "      <th>QDifficulty_3</th>\n",
       "      <th>QDifficulty_4</th>\n",
       "      <th>QDifficulty_5</th>\n",
       "      <th>QWorkload_1</th>\n",
       "      <th>QWorkload_2</th>\n",
       "      <th>QWorkload_3</th>\n",
       "      <th>QWorkload_4</th>\n",
       "      <th>QWorkload_5</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sem_Average</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTSCI</td>\n",
       "      <td>270</td>\n",
       "      <td>HISTSCI-270</td>\n",
       "      <td>58523</td>\n",
       "      <td>2697</td>\n",
       "      <td>Spring '12</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>4.67</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6</td>\n",
       "      <td>50.00</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>Lemov</td>\n",
       "      <td>79de794d3e2e19eb71a2033b0ec0b76d</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.33</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'This course is a perfect example of what gr...</td>\n",
       "      <td>4.226350</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '14</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>4.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.50</td>\n",
       "      <td>13</td>\n",
       "      <td>76.92</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.60</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'The class has a fairly high work load, but ...</td>\n",
       "      <td>4.244370</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '13</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>13</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'Philosophy of the State with Dr. Chen offer...</td>\n",
       "      <td>4.256888</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '12</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.47</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.47</td>\n",
       "      <td>15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.18</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'This was by far my favorite course. Dr. Che...</td>\n",
       "      <td>4.190299</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '11</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.62</td>\n",
       "      <td>13</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.92</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'Be prepared to read', u'Discussions were gr...</td>\n",
       "      <td>4.185893</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  C_Department C_Number        Course  C_CatNum  C_ID  C_Semester  C_Year  C_Term  C_Overall  C_Workload  C_Difficulty  C_Recommendation  C_Enrollment  C_ResponseRate  I_First I_Last                              I_ID  I_Overall  I_EffectiveLectures  I_Accessible  I_GeneratesEnthusiasm  I_EncouragesParticipation  I_UsefulFeedback  I_ReturnsAssignmentsTimely  QOverall_1  QOverall_2  QOverall_3  QOverall_4  QOverall_5  QDifficulty_1  QDifficulty_2  QDifficulty_3  QDifficulty_4  QDifficulty_5  \\\n",
       "0      HISTSCI      270   HISTSCI-270     58523  2697  Spring '12    2011       2       4.67        2.33          3.33              5.00             6           50.00  Rebecca  Lemov  79de794d3e2e19eb71a2033b0ec0b76d       4.67                 4.33          4.00                   4.33                       5.00              4.50                        4.00           0           0           0           1           2              0              0              2              1              0   \n",
       "1        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '14    2014       1       4.10        7.10           NaN              3.50            13           76.92     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       4.50                 4.60          3.70                   3.90                       3.90              4.10                        4.60           0           1           1           4           4              0              1              2              7              3   \n",
       "2        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '13    2013       1       3.50        2.60          3.90              3.20            13          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       3.80                 4.00          2.50                   3.50                       4.10              4.30                        4.40           0           1           1           4           4              0              1              2              7              3   \n",
       "3        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '12    2012       1       3.73        2.47          3.67              3.47            15          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       3.87                 4.33          2.64                   3.93                       4.18              3.64                        3.82           0           1           1           4           4              0              1              2              7              3   \n",
       "4        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '11    2011       1       3.85        2.00          3.54              3.62            13          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       4.08                 3.75          3.31                   3.92                       4.46              4.23                        4.08           0           1           1           4           4              0              1              2              7              3   \n",
       "\n",
       "   QWorkload_1  QWorkload_2  QWorkload_3  QWorkload_4  QWorkload_5                                           Comments  Sem_Average Positive  \n",
       "0            0            2            1            0            0  [u'This course is a perfect example of what gr...     4.226350     True  \n",
       "1            1            3            4            1            0  [u'The class has a fairly high work load, but ...     4.244370    False  \n",
       "2            1            3            4            1            0  [u'Philosophy of the State with Dr. Chen offer...     4.256888    False  \n",
       "3            1            3            4            1            0  [u'This was by far my favorite course. Dr. Che...     4.190299    False  \n",
       "4            1            3            4            1            0  [u'Be prepared to read', u'Discussions were gr...     4.185893    False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf=pd.read_csv(\"bigdf.csv\")\n",
    "bigdf.reset_index(drop=True)\n",
    "bigdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_comments(commentsListAsString):\n",
    "    if type(commentsListAsString) != str:\n",
    "        return \"\"\n",
    "    else:\n",
    "        allComments = ast.literal_eval(commentsListAsString)\n",
    "        if len(allComments) >= MIN_PER_COURSESEM_REVIEWS:\n",
    "            return \" \".join(np.random.choice(allComments, MIN_PER_COURSESEM_REVIEWS, replace=False))\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "subdf = bigdf[['Course','C_Semester','Comments', 'Positive']].dropna()\n",
    "subdf[\"Sampled_Comments\"] = subdf.Comments.map(sample_comments)\n",
    "subdf = subdf[subdf.Sampled_Comments != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Course</th>\n",
       "      <th>C_Semester</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Sampled_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>Fall '14</td>\n",
       "      <td>[u'The class has a fairly high work load, but ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Lot of work/reading but very rewarding. You re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>Fall '13</td>\n",
       "      <td>[u'Philosophy of the State with Dr. Chen offer...</td>\n",
       "      <td>False</td>\n",
       "      <td>This class is always rumored to be very diffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>Fall '12</td>\n",
       "      <td>[u'This was by far my favorite course. Dr. Che...</td>\n",
       "      <td>False</td>\n",
       "      <td>Take this class if you are interested in the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>Fall '11</td>\n",
       "      <td>[u'Be prepared to read', u'Discussions were gr...</td>\n",
       "      <td>False</td>\n",
       "      <td>You'll have to think deeply about your opinion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EXPOS-20.133</td>\n",
       "      <td>Spring '12</td>\n",
       "      <td>[u'This is a fantastic course if you have an i...</td>\n",
       "      <td>False</td>\n",
       "      <td>This class is really difficult, but if you enj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EXPOS-20.133</td>\n",
       "      <td>Spring '14</td>\n",
       "      <td>[u'This is not a bad Expos course. Dr. Chen is...</td>\n",
       "      <td>False</td>\n",
       "      <td>Be ready to speak up in class If you like phil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EXPOS-20.131</td>\n",
       "      <td>Fall '14</td>\n",
       "      <td>[u'If you are interested in political philosop...</td>\n",
       "      <td>False</td>\n",
       "      <td>The preceptor gives good feedback. This course...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EXPOS-20.131</td>\n",
       "      <td>Fall '13</td>\n",
       "      <td>[u'This class involves quite a bit of reading,...</td>\n",
       "      <td>False</td>\n",
       "      <td>If you have any interest at all in philosophy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EXPOS-20.131</td>\n",
       "      <td>Fall '12</td>\n",
       "      <td>[u\"A basic, but thorough, understanding of phi...</td>\n",
       "      <td>False</td>\n",
       "      <td>In this class, you will get to work through a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EXPOS-20.131</td>\n",
       "      <td>Fall '11</td>\n",
       "      <td>[u'It is a great class, which combines develop...</td>\n",
       "      <td>True</td>\n",
       "      <td>If you decide to take this course, it will be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>EXPOS-20.134</td>\n",
       "      <td>Spring '12</td>\n",
       "      <td>[u\"It won't always be easy, but this class def...</td>\n",
       "      <td>False</td>\n",
       "      <td>Although there is a lot of work and the materi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EXPOS-20.134</td>\n",
       "      <td>Spring '14</td>\n",
       "      <td>[u\"The reading material is interesting and cha...</td>\n",
       "      <td>True</td>\n",
       "      <td>While this is probably one of the hardest and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Course  C_Semester                                           Comments Positive                                   Sampled_Comments\n",
       "1   EXPOS-20.132    Fall '14  [u'The class has a fairly high work load, but ...    False  Lot of work/reading but very rewarding. You re...\n",
       "2   EXPOS-20.132    Fall '13  [u'Philosophy of the State with Dr. Chen offer...    False  This class is always rumored to be very diffic...\n",
       "3   EXPOS-20.132    Fall '12  [u'This was by far my favorite course. Dr. Che...    False  Take this class if you are interested in the c...\n",
       "4   EXPOS-20.132    Fall '11  [u'Be prepared to read', u'Discussions were gr...    False  You'll have to think deeply about your opinion...\n",
       "6   EXPOS-20.133  Spring '12  [u'This is a fantastic course if you have an i...    False  This class is really difficult, but if you enj...\n",
       "7   EXPOS-20.133  Spring '14  [u'This is not a bad Expos course. Dr. Chen is...    False  Be ready to speak up in class If you like phil...\n",
       "13  EXPOS-20.131    Fall '14  [u'If you are interested in political philosop...    False  The preceptor gives good feedback. This course...\n",
       "14  EXPOS-20.131    Fall '13  [u'This class involves quite a bit of reading,...    False  If you have any interest at all in philosophy,...\n",
       "15  EXPOS-20.131    Fall '12  [u\"A basic, but thorough, understanding of phi...    False  In this class, you will get to work through a ...\n",
       "16  EXPOS-20.131    Fall '11  [u'It is a great class, which combines develop...     True  If you decide to take this course, it will be ...\n",
       "19  EXPOS-20.134  Spring '12  [u\"It won't always be easy, but this class def...    False  Although there is a lot of work and the materi...\n",
       "20  EXPOS-20.134  Spring '14  [u\"The reading material is interesting and cha...     True  While this is probably one of the hardest and ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our comments dataframe, subdf, to a spark dataframe for text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/spark\n"
     ]
    }
   ],
   "source": [
    "#setup spark\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()\n",
    "sys.version\n",
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS\n",
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")\n",
    "#Useless verbs courtesy of: http://mbweston.com/2012/11/26/writing-editing-find-and-eliminate-useless-verbs/\n",
    "uselessverbs = ['be','is','are','be','was','were','been','being',\n",
    "                'go','goes','went','gone','going','put','puts','putting',\n",
    "                'do','does','did','done','doing',\n",
    "                'come','comes','came','coming',\n",
    "                'have','have','has','had','having',\n",
    "                'can','could','begin','begins','began','begun','beginning',\n",
    "                'seem','seems','seemed','seeming',\n",
    "                'get','got','gotten','getting',\n",
    "                'become','became','becoming']\n",
    "import re\n",
    "regex1=re.compile(r\"\\.{2,}\")\n",
    "regex2=re.compile(r\"\\-{2,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a get parts function to parse the language in the comments. This customize get_parts function returns lists of the nouns, adjectives and verbs in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parts(thetext):\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    verbs = []\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        verbs.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "                elif token[1] in ['VB','VBP','VBZ','VBG','VBD','VBN']:\n",
    "                    if token[4] in stopwords or token[4] in uselessverbs or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    verbs[i].append(token[4])\n",
    "    out=zip(nouns, descriptives,verbs)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    verbs2 = []\n",
    "    for n,d,v in out:\n",
    "        if len(n)!=0 and len(d)!=0 and len(v)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "            verbs2.append(v)\n",
    "    return nouns2, descriptives2, verbs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------+--------+--------------------+\n",
      "|      Course|C_Semester|            Comments|Positive|    Sampled_Comments|\n",
      "+------------+----------+--------------------+--------+--------------------+\n",
      "|EXPOS-20.132|  Fall '14|[u'The class has ...|   false|Lot of work/readi...|\n",
      "|EXPOS-20.132|  Fall '13|[u'Philosophy of ...|   false|This class is alw...|\n",
      "|EXPOS-20.132|  Fall '12|[u'This was by fa...|   false|Take this class i...|\n",
      "|EXPOS-20.132|  Fall '11|[u'Be prepared to...|   false|You'll have to th...|\n",
      "|EXPOS-20.133|Spring '12|[u'This is a fant...|   false|This class is rea...|\n",
      "+------------+----------+--------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subdf = sqlsc.createDataFrame(subdf)\n",
    "subdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[u'quality', u'instruction', u'course', u'instructor', u'time'],\n",
       "   [u'paper', u'family', u'state'],\n",
       "   [u'lot', u'work', u'summary', u'assignment'],\n",
       "   [u'class', u'work', u'load', u'writer'],\n",
       "   [u'course'],\n",
       "   [u'subject', u'course', u'overall', u'expectation'],\n",
       "   [u'purpose',\n",
       "    u'class',\n",
       "    u'freshman',\n",
       "    u'writing',\n",
       "    u'skill',\n",
       "    u'class',\n",
       "    u'resource',\n",
       "    u'purpose'],\n",
       "   [u'work', u'effort', u'essay'],\n",
       "   [u'essay', u'topic', u'guidance'],\n",
       "   [u'feedback', u'draft', u'draft'],\n",
       "   [u'feedback', u'office', u'hour'],\n",
       "   [u'peer', u'resource', u'guidance', u'class'],\n",
       "   [u'class', u'requirement']],\n",
       "  [[u'generous'],\n",
       "   [u'final'],\n",
       "   [u'prepared', u'dense', u'philosophical', u'online'],\n",
       "   [u'high', u'helpful'],\n",
       "   [u'unlikely'],\n",
       "   [u'passionate'],\n",
       "   [u'useful'],\n",
       "   [u'manageable'],\n",
       "   [u'reasonable', u'little'],\n",
       "   [u'vague', u'unhelpful', u'final'],\n",
       "   [u'difficult', u'constructive', u'accessible'],\n",
       "   [u'writing'],\n",
       "   [u'particular']],\n",
       "  [[u'receive'],\n",
       "   [u'focuse'],\n",
       "   [u'read', u'post', u'work'],\n",
       "   [u'help', u'improve'],\n",
       "   [u'recommend'],\n",
       "   [u'meet'],\n",
       "   [u'educate', u'provide', u'fulfill'],\n",
       "   [u'relate'],\n",
       "   [u'provide'],\n",
       "   [u'provide'],\n",
       "   [u'ask'],\n",
       "   [u'obtain'],\n",
       "   [u'recommend', u'fulfill']])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_parts = subdf.rdd.map(lambda r: get_parts(r.Sampled_Comments))\n",
    "comment_parts.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 171 ms, sys: 37.4 ms, total: 208 ms\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsedcomments=comment_parts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin our text analysis with an LDA of the nouns in the comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'quality', u'instruction', u'course', u'instructor', u'time'],\n",
       "  [u'paper', u'family', u'state'],\n",
       "  [u'lot', u'work', u'summary', u'assignment'],\n",
       "  [u'class', u'work', u'load', u'writer'],\n",
       "  [u'course'],\n",
       "  [u'subject', u'course', u'overall', u'expectation'],\n",
       "  [u'purpose',\n",
       "   u'class',\n",
       "   u'freshman',\n",
       "   u'writing',\n",
       "   u'skill',\n",
       "   u'class',\n",
       "   u'resource',\n",
       "   u'purpose'],\n",
       "  [u'work', u'effort', u'essay'],\n",
       "  [u'essay', u'topic', u'guidance'],\n",
       "  [u'feedback', u'draft', u'draft'],\n",
       "  [u'feedback', u'office', u'hour'],\n",
       "  [u'peer', u'resource', u'guidance', u'class'],\n",
       "  [u'class', u'requirement']],\n",
       " [[u'class'],\n",
       "  [u'lot', u'philosophy', u'professor', u'lot'],\n",
       "  [u'course', u'lot', u'reading', u'writing'],\n",
       "  [u'philosophy',\n",
       "   u'stimulating',\n",
       "   u'class',\n",
       "   u'discussion',\n",
       "   u'variety',\n",
       "   u'topic',\n",
       "   u'student',\n",
       "   u'share'],\n",
       "  [u'class', u'lot'],\n",
       "  [u'class', u'improvement', u'writing', u'understanding', u'writing'],\n",
       "  [u'student', u'class', u'philosophy', u'level', u'discussion'],\n",
       "  [u'class', u'philosophy', u'introduction', u'course', u'ton', u'student'],\n",
       "  [u'class', u'discussion', u'class', u'lot', u'philosophy']],\n",
       " [[u'reading', u'discussion'],\n",
       "  [u'writing'],\n",
       "  [u'class', u'class', u'reading', u'number'],\n",
       "  [u'time', u'class', u'lot', u'class', u'discussion'],\n",
       "  [u'day', u'material'],\n",
       "  [u'kind', u'class'],\n",
       "  [u'decision', u'course']]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e[0] for e in parsedcomments[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'quality', u'instruction', u'course', u'instructor', u'time'],\n",
       " [u'paper', u'family', u'state'],\n",
       " [u'lot', u'work', u'summary', u'assignment'],\n",
       " [u'class', u'work', u'load', u'writer'],\n",
       " [u'course']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldadatardd=sc.parallelize([ele[0] for ele in parsedcomments]).flatMap(lambda l: l)\n",
    "ldadatardd.cache()\n",
    "ldadatardd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'quality', u'instruction', u'course', u'instructor', u'time']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldadatardd.flatMap(lambda word: word).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabtups = (ldadatardd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab=vocabtups.collectAsMap()\n",
    "id2word=vocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'stuff\\xe2\\u20ac', u'databasis', 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0], vocab.keys()[5], vocab[vocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4322"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def helperfunction(element):\n",
    "    d = defaultdict(int)\n",
    "    for k in element:\n",
    "        d[vocab[k]] += 1\n",
    "    return d.items()\n",
    "documents = ldadatardd.map(lambda w: helperfunction(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1315, 1), (2575, 1), (2959, 1), (1030, 1), (3615, 1)],\n",
       " [(2243, 1), (3798, 1), (1622, 1)],\n",
       " [(1619, 1), (3644, 1), (173, 1), (2894, 1)],\n",
       " [(2675, 1), (4291, 1), (173, 1), (2787, 1)],\n",
       " [(1315, 1)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=documents.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda2 = gensim.models.ldamodel.LdaModel(corpus = corpus, num_topics = 2, id2word=id2word, chunksize=200, passes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we print the topics we find using LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first topic (let us call this Topic 0) includes the combination of words: \n",
    "\n",
    "- class, lot, time, work, way, professor, problem, section, exam, and fun.\n",
    "\n",
    "\n",
    "The second topic (let us call this Topic 1) includes the following combination of words: \n",
    "\n",
    "- course, material, lecture, student, topic, reading, person, science, year, and lecturer.\n",
    "\n",
    "\n",
    "Topic 0 seems to encompass the more interactive, qualitative, personable aspects of the course with key terms including class, professor, problem, section, and fun. Topic 1, by contrast, seems to encompass the more solitary, logistical, factual aspects of a course that a student experiences with key terms including material, lecture, student, topic, reading, year, and lecturer. One thing worth noticing is that Topic 1 includes \"course\" as a key term while Topic 0 includes \"class\". While \"course\" and \"class\" are often used interchangably in language, \"class\" arguably connotes a more personal, interactive experience than \"course\", which is more administrative and logistical and more likely to be used as an umbrella term for everything from everyday class to homework. This would support our separation of Topic 0 and 1 into more interactive/personable aspects and more logistical/solitary aspects, respectively.\n",
    "\n",
    "In order to further evaluate our intial hypothesis that course reviews are split along two topics (interactive, qualitiative, personable aspects v. solitary, logistical aspects), we will output the words of some sentences, along with the probability of the sentence belonging to Topic 0 and Topic 1, to qualitatively check that our topics are reasonable and supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for bow in corpus[0:1200:60]:\n",
    "    print bow\n",
    "    print lda2.get_document_topics(bow)\n",
    "    print \" \".join([id2word[e[0]] for e in bow])\n",
    "    print \"==========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"sentences\" (or bag-of-words) which have a much greater probability of belonging to Topic 0 include:\n",
    "- class writer load work\n",
    "- class bit pass work\n",
    "- class cost intervention u'very benefit reform lot issue\n",
    "- class student success belief stand risk course question ability\n",
    "- class kink\n",
    "- way professor class discussion material\n",
    "- food professor person class\n",
    "- class section discussion student\n",
    "- time homework week\n",
    "\n",
    "The words in these sentences are more descriptive and relate to more creative, interactive, person-to-person aspects of a course. Specifically, \"writer\", \"intervention\", \"reform\", \"issue\", \"success\", \"belief\", \"stand\", \"risk\", \"question\", \"kink\", and \"discussion\" all imply rich and diverse elements of the course experience. As a Harvard student, I know that the word \"section\" also implies discussion and collaboration since sections for Harvard classes provide an opportunity outside of lecture to engage more closely with course material and consist of tight-knit groups.\n",
    "\n",
    "\n",
    "The \"setences\" which have a much greater probability of belonging to Topic 1 include:\n",
    "- concept background\n",
    "- reading\n",
    "- thought-provoking discussion debate material staff teaching\n",
    "\n",
    "The function below transforms X-col (which consists of word-based \"sentences\" (bag-of-words or \"documents\")) using the vectorizer which is also a parameter.\n",
    "- time bit course u\"must\n",
    "- career course regret\n",
    "- education perspective lot debate history\n",
    "\n",
    "Words in these sentences that are not present in the previous cluster of setences and that stand out as implying more impersonal, logistical, or practical aspects of a course include \"concept\", \"background\", \"reading\", \"material\", \"time\", \"must\", \"education\", \"history\", and arguably \"staff\" (since \"staff\" is a somewhat impersonal way to refer to professors and teaching fellows). Although words like \"thought-provoking\", \"discussion\", \"debate\" and \"teaching\" do appear, it is worthwhile to note that the sentence/bag-of-words in which they appear still has a relatively high probability of belonging to Topic 0 (~35%).\n",
    "\n",
    "The \"sentences\" which have more equal probabilities of belonging to Topic 0 or 1 include:\n",
    "- time assignment philosophy night\n",
    "- course lot\n",
    "- chance education course lot style\n",
    "- student reason\n",
    "- way class overview study\n",
    "\n",
    "For sentences/bag-of-words with relatively equal probabilities of belonging to Topic 0 and Topic 1, we can observe both words implying more interactive, creative aspects (\"philosophy\", \"style\", \"student\", \"reason\") and words implying more logistical aspects (\"time\", \"assignment\", \"overview\", \"study\").\n",
    "\n",
    "\n",
    "From our analysis of the topic probabilities and bag-of-words above, there appears to be evidence to support our initial hypothesis that course reviews are split along two topics: Topic 0, which includes more interactive, qualitative, personable aspects of a course, and Topic 1, which includes more solitary, logistical aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DOs:\n",
    "- We can consider doing \"verbs\" (use TextBlob)\n",
    "- Detect \"not\" before adjectives, this shouldn't be too difficult\n",
    "- Text before pushing\n",
    "- Look at differences across departments (Jesse/Andrew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us now continue with a sentiment analysis of the adjectives in the comments using Naive Bayes. We begin by extracting the adjectives as we did before with the nouns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbdatardd=sc.parallelize([ele[1] for ele in parsedcomments])\n",
    "nbdatardd.cache()\n",
    "nbdatardd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adjvocabtups = (nbdatardd.flatMap(lambda l: l).flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ").cache()\n",
    "adjvocab=adjvocabtups.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(adjvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to flatten the all of the adjectives for a particular semester of comments of a course into a single document and then make an array of these documents for get our adjective \"features\" and retrieve our response array, comprised of the positive column of our subdf, as this is the response variable we are trying to predict. The length of these two arrays should be equal if we do this correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "Xarraypre=nbdatardd.map(lambda l: \" \".join(list(itertools.chain.from_iterable(l))))\n",
    "Xarray=Xarraypre.collect()\n",
    "resparray = subdf.rdd.map(lambda r: r.Positive).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Xarray), len(resparray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use mask to create a train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "itrain, itest = train_test_split(xrange(len(Xarray)), train_size=0.7)\n",
    "mask=np.ones(len(Xarray), dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then transform the adjectives into a bag of words representation and use a vectorizer to create a feature and write some support functions for Naive Bayes analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_xy(X_col, y_col, vectorizer):\n",
    "    X = vectorizer.fit_transform(X_col)\n",
    "    y = y_col\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan on using log-likelyhood as a scoring metric and write a support function to be able to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    negatives = y == False\n",
    "    positives = ~negatives\n",
    "    return prob[negatives, False].sum() + prob[positives, True].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to estimate the cross-validated score (given a classifier, data, and a scoring function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cv_score(clf, x, y, score_func, nfold=5):\n",
    "    \"\"\"\n",
    "    Uses 5-fold cross validation to estimate a score of a classifier\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    clf : Classifier object\n",
    "    x : Input feature vector\n",
    "    y : Input class labels\n",
    "    score_func : Function like log_likelihood, that takes (clf, x, y) as input,\n",
    "                 and returns a score\n",
    "                 \n",
    "    Returns\n",
    "    -------\n",
    "    The average score obtained by splitting (x, y) into 5 folds of training and \n",
    "    test sets, fitting on the training set, and evaluating score_func on the test set\n",
    "    \n",
    "    Examples\n",
    "    cv_score(clf, x, y, log_likelihood)\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf, x[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a usseful function for visualizing the calibration of a probabilistic classifier (in order to recognize whether our classifier is over-confident or under-confident)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calibration_plot(clf, xtest, ytest):\n",
    "    prob = clf.predict_proba(xtest)[:, 1]\n",
    "    outcome = ytest\n",
    "    data = pd.DataFrame(dict(prob=prob, outcome=outcome))\n",
    "\n",
    "    #group outcomes into bins of similar probability\n",
    "    bins = np.linspace(0, 1, 20)\n",
    "    cuts = pd.cut(prob, bins)\n",
    "    binwidth = bins[1] - bins[0]\n",
    "    \n",
    "    #freshness ratio and number of examples in each bin\n",
    "    cal = data.groupby(cuts).outcome.agg(['mean', 'count'])\n",
    "    cal['pmid'] = (bins[:-1] + bins[1:]) / 2\n",
    "    cal['sig'] = np.sqrt(cal.pmid * (1 - cal.pmid) / cal['count'])\n",
    "        \n",
    "    #the calibration plot\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    p = plt.errorbar(cal.pmid, cal['mean'], cal['sig'])\n",
    "    plt.plot(cal.pmid, cal.pmid, linestyle='--', lw=1, color='k')\n",
    "    plt.ylabel(\"Empirical P(+)\")\n",
    "    \n",
    "    #the distribution of P(+)\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0), sharex=ax)\n",
    "    \n",
    "    plt.bar(left=cal.pmid - binwidth / 2, height=cal['count'],\n",
    "            width=.95 * (bins[1] - bins[0]),\n",
    "            fc=p[0].get_color())\n",
    "    \n",
    "    plt.xlabel(\"Predicted P(+)\")\n",
    "    plt.ylabel(\"Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert Xarray and Resparray into numpy arrays for use with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.array(Xarray)\n",
    "y=np.array(resparray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a cross-validation loop to find the best hyper-parameters for our Naive Bayes analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the grid of parameters to search over\n",
    "alphas = [0, .1, 1, 5, 10, 50]\n",
    "min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:\n",
    "    for min_df in min_dfs:         \n",
    "        vectorizer = CountVectorizer(vocabulary = adjvocab, min_df = min_df)       \n",
    "        Xthis, ythis = make_xy(X, y, vectorizer)\n",
    "        Xtrainthis=Xthis[mask]\n",
    "        ytrainthis=ythis[mask]\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "        if cvscore > maxscore:\n",
    "            maxscore = cvscore\n",
    "            best_alpha, best_min_df = alpha, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"alpha: %f\" % best_alpha\n",
    "print \"min_df: %f\" % best_min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have determined the best parameters, we are ready to run the Naive Bayes classifier and create a calibration plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary = adjvocab, min_df=best_min_df)\n",
    "X2, y2 = make_xy(X, y, vectorizer)\n",
    "xtrain=X2[mask]\n",
    "ytrain=y2[mask]\n",
    "xtest=X2[~mask]\n",
    "ytest=y2[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "# Your code here. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "calibration_plot(clf, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbdatardd_verbs=sc.parallelize([ele[2] for ele in parsedcomments])\n",
    "nbdatardd_verbs.cache()\n",
    "nbdatardd_verbs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbvocabtups = (nbdatardd_verbs.flatMap(lambda l: l).flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    ").cache()\n",
    "verbvocab=verbvocabtups.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the grid of parameters to search over\n",
    "alphas = [0, .1, 1, 5, 10, 50]\n",
    "min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:\n",
    "    for min_df in min_dfs:         \n",
    "        vectorizer = CountVectorizer(vocabulary = verbvocab, min_df = min_df)       \n",
    "        Xthis, ythis = make_xy(X, y, vectorizer)\n",
    "        Xtrainthis=Xthis[mask]\n",
    "        ytrainthis=ythis[mask]\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "        if cvscore > maxscore:\n",
    "            maxscore = cvscore\n",
    "            best_alpha, best_min_df = alpha, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"alpha: %f\" % best_alpha\n",
    "print \"min_df: %f\" % best_min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary = verbvocab, min_df=best_min_df)\n",
    "X2, y2 = make_xy(X, y, vectorizer)\n",
    "xtrain=X2[mask]\n",
    "ytrain=y2[mask]\n",
    "xtest=X2[~mask]\n",
    "ytest=y2[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "# Your code here. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "calibration_plot(clf, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
