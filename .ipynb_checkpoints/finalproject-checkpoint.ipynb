{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide Me Through the Q!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q](images/q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Table of Contents\n",
    "\n",
    "* [Part 1: Creating the Data Set](#Part-1:-Creating-the-Data-Set)\n",
    "    * [1.1 Concatenating the JSON Files](#1.1-Concatenating-the-JSON-Files)\n",
    "    * [1.2 Producing the Dataframe](#1.2-Producing-the-Dataframe)\n",
    "    * [1.3 Cleaning](#1.3-Cleaning)\n",
    "    * [1.4 Adding an Indicator for Positive Reviews](#1.4-Adding-an-Indicator-for-Positive-Reviews)\n",
    "* [Part 2: Numerical Analysis](#Part-2:-Numerical-Analysis)\n",
    "    * [2.1 Statistics and Trends](#2.1-Statistics-and-Trends)\n",
    "    * [2.2 Training and Testing](#2.2-Training-and-Testing)\n",
    "        * [2.21 Linear SVM](#2.21-Linear-SVM)\n",
    "* [Part 3: Text Analysis](#Part-3:-Text-Analysis)\n",
    "* [References](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Concatenating the JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q data we received consisted of about 2300 JSON files, each one representing a unique course's historical Q scores for each available semester. Out first task was to combine these individual JSON files into a single JSON Lines file; the JSON files were stored locally outside of the repository directory, and we used the `shututil` and `glob` libraries to concatenate these into `qall.jsonl`. Because the individual JSON files are local, we used the `os.path` library to check if the JSONL file already exists and to prevent the code from running if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "# Prevents code from running if the file already exists.\n",
    "if not os.path.exists(\"qall.jsonl\"):\n",
    "\n",
    "    # Opens the JSONL file as output.\n",
    "    with open(\"qall.jsonl\", \"wb\") as outfile:\n",
    "    \n",
    "        # Iterates over the JSON files.\n",
    "        for filename in glob.glob(\"../Q/*.json\"):\n",
    "        \n",
    "            # Opens the current JSON file as input.\n",
    "            with open(filename, \"rb\") as readfile:\n",
    "            \n",
    "                # Copies the JSON file to the JSONL file.\n",
    "                shutil.copyfileobj(readfile, outfile)\n",
    "            \n",
    "                # Adds a new line as a delimiter.\n",
    "                outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Producing the Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each JSON file was structured using multiple nested dictionaries, with the following format:\n",
    "\n",
    "* `scores`: a dictionary of mostly numerical data, separated by semester\n",
    "    * `*semester*`: a dictionary of course and instructor scores\n",
    "        * `course_score`: a dictionary of course info, ratings, and rating breakdowns\n",
    "            * `overall`: a float of mean overall rating\n",
    "            * `workload`: a float of mean workload rating\n",
    "            * `difficulty`: a float of mean difficulty rating\n",
    "            * `recommendation`: a float of mean recommendation rating\n",
    "            * `enrollment`: an integer of class enrollment\n",
    "            * `reponse rate`: a float of the proportion of students who reviewed the course\n",
    "            * `QCourseOverall`: \n",
    "                * `number`: the string concatenation of department and course number, i.e. \"COMPSCI 109\"\n",
    "                * `course_id`: an integer identifier for my.harvard.edu\n",
    "                * `cat_num`: an integer indentifier for the Harvard Course Catalog\n",
    "                * `1s`: an integer count of reviews with rating 1 for this category\n",
    "                * `2s`: analagous, with 2s\n",
    "                * `3s`: analagous, with 3s\n",
    "                * `4s`: analagous, with 4s\n",
    "                * `5s`: analagous, with 5s\n",
    "            * `QDifficulty`: analagous to `QCourseOverall`, with difficulty ratings.\n",
    "            * `QWorkload`: analagous to `QCourseOverall`, with workload ratings.\n",
    "        * `instructor_scores`: a list of length 1 that contains a dictionary of instructor info and ratings\n",
    "            * `number`: identical to that within `QCourseOverall`\n",
    "            * `cat_num`: identical to that within `QCourseOverall`\n",
    "            * `course_id`: identical to that within `QCourseOverall`\n",
    "            * `year`: an int of the first of the two years represented in the given school year\n",
    "            * `term`: an int with value 1 if the course took place in the Fall, 2 if Spring\n",
    "            * `id`: a string indentifier of the professor\n",
    "            * `first`: a string of the first name of the professor\n",
    "            * `last`: a string of the last name\n",
    "            * `InstructorOverall`: a float of mean overall instructor rating\n",
    "            * `EffectiveLecturesorPresentations`: a float of mean lecture rating\n",
    "            * `AccessibleOutsideClass`: a float of mean accessibility rating\n",
    "            * `GeneratesEnthusiasm`: a float of mean enthusiasm rating\n",
    "            * `FacilitatesDiscussionEncouragesParticipation`: a float of mean facilitation rating\n",
    "            * `GivesUsefulFeedback`: a float of mean feedback rating\n",
    "            * `ReturnsAssignmentsinTimelyFashion`: a float of mean timeliness rating\n",
    "* `comments`: a dictionary of textual reviews, separated by semester into dictionaries\n",
    "    * `*semester*`: a dictionary of comments for the given semester\n",
    "        * `comments`: a list of strings that represent comments\n",
    "* `mostRecentQ`: identical to the most recent semester data in `scores`\n",
    "* `success`: indicates if the JSON was successfully retrieved\n",
    "\n",
    "Any mean rating has range `1.0-5.0`, and response rate has a range `0.0-1.0`. The entries labeled `*semester*` can occur any number of times and have as a key the semester in which it took place (i.e. `Fall '12`).\n",
    "\n",
    "We wanted our first dataframe `bigdf` to have a single row for each instance of a course (`COMPSCI 109` in `Fall '13` and `COMPSCI 109` in `Fall '14` have separate rows). This versatile dataframe could be adjusted in a number of different ways to suit the purposes of our data analysis. One caveat was a lack of course or instructor data, or both! To handle for this, we first created an empty dataframe with specified columns; then, we loaded the data for each cousrse using the `json` library, created a minimal dictionary for each semester, added further semester data to the dictionary conditional upon the data's existence, then appended that dictionary to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Creates the dataframe with columns specified.\n",
    "bigdf = pd.DataFrame(columns=[\"C_Department\",\"C_Number\", \"Course\", \"C_CatNum\",\"C_ID\",\"C_Semester\",\"C_Year\",\"C_Term\",\n",
    "                              \"C_Overall\",\"C_Workload\",\"C_Difficulty\",\"C_Recommendation\",\"C_Enrollment\",\"C_ResponseRate\",\n",
    "                              \"I_First\",\"I_Last\",\"I_ID\",\n",
    "                              \"I_Overall\",\"I_EffectiveLectures\",\"I_Accessible\",\"I_GeneratesEnthusiasm\",\n",
    "                              \"I_EncouragesParticipation\",\"I_UsefulFeedback\",\"I_ReturnsAssignmentsTimely\",\n",
    "                              \"QOverall_1\",\"QOverall_2\",\"QOverall_3\",\"QOverall_4\",\"QOverall_5\",\n",
    "                              \"QDifficulty_1\",\"QDifficulty_2\",\"QDifficulty_3\",\"QDifficulty_4\",\"QDifficulty_5\",\n",
    "                              \"QWorkload_1\",\"QWorkload_2\",\"QWorkload_3\",\"QWorkload_4\",\"QWorkload_5\",\n",
    "                              \"Comments\"])\n",
    "\n",
    "# Opens the JSONL file as the input.\n",
    "with open(\"qall.jsonl\") as data_file:\n",
    "\n",
    "    # Iterates over each line.\n",
    "    for line in data_file:\n",
    "\n",
    "        # Loads the JSON.\n",
    "        alldata = json.loads(line)\n",
    "\n",
    "        # Iterates over each semester for the course.\n",
    "        for semester in alldata[\"scores\"]:\n",
    "\n",
    "            # Retrieves the semester data.\n",
    "            semdata = alldata[\"scores\"][semester]\n",
    "\n",
    "            # Retrieves the scores or comments if they exist, else None.\n",
    "            cscores = semdata[\"course_score\"] if \"course_score\" in semdata else None\n",
    "            iscores = semdata[\"instructor_scores\"][0] if \"instructor_scores\" in semdata else None\n",
    "            comments = alldata[\"comments\"][semester][\"comments\"] if semester in alldata[\"comments\"] else None\n",
    "\n",
    "            # Creates a dictionary that will be appended as a row to the dataframe.\n",
    "            semdatadict = {\"C_Semester\": semester, \"Comments\": comments}\n",
    "\n",
    "            # If the course scores exist...\n",
    "            if cscores != None:\n",
    "                \n",
    "                # Retrieves the course name and separates it into department and number.\n",
    "                course = str(cscores[\"QCourseOverall\"][\"number\"]).split()\n",
    "                if course[0] == \"None\":\n",
    "                    course = [None, None]\n",
    "                    fullname = \"NODEPT-NOCOURSENUMBER\"\n",
    "                else:\n",
    "                    fullname = course[0] + \"-\" + course[1]\n",
    "                    \n",
    "                # Updates the dictionary with the data.\n",
    "                semdatadict.update({\"C_Department\": course[0],\n",
    "                                    \"C_Number\": course[1],\n",
    "                                    \"Course\": fullname,\n",
    "                                    \"C_CatNum\": cscores[\"QCourseOverall\"][\"cat_num\"],\n",
    "                                    \"C_ID\": cscores[\"QCourseOverall\"][\"course_id\"],\n",
    "                                    \"C_Overall\": cscores[\"overall\"],\n",
    "                                    \"C_Workload\": cscores[\"workload\"],\n",
    "                                    \"C_Difficulty\": cscores[\"difficulty\"],\n",
    "                                    \"C_Recommendation\": cscores[\"recommendation\"],\n",
    "                                    \"C_Enrollment\": cscores[\"enrollment\"],\n",
    "                                    \"C_ResponseRate\": cscores[\"response rate\"],\n",
    "                                    \"QOverall_1\": cscores[\"QCourseOverall\"][\"1s\"],\n",
    "                                    \"QOverall_2\": cscores[\"QCourseOverall\"][\"2s\"],\n",
    "                                    \"QOverall_3\": cscores[\"QCourseOverall\"][\"3s\"],\n",
    "                                    \"QOverall_4\": cscores[\"QCourseOverall\"][\"4s\"],\n",
    "                                    \"QOverall_5\": cscores[\"QCourseOverall\"][\"5s\"],\n",
    "                                    \"QDifficulty_1\": cscores[\"QDifficulty\"][\"1s\"],\n",
    "                                    \"QDifficulty_2\": cscores[\"QDifficulty\"][\"2s\"],\n",
    "                                    \"QDifficulty_3\": cscores[\"QDifficulty\"][\"3s\"],\n",
    "                                    \"QDifficulty_4\": cscores[\"QDifficulty\"][\"4s\"],\n",
    "                                    \"QDifficulty_5\": cscores[\"QDifficulty\"][\"5s\"],\n",
    "                                    \"QWorkload_1\": cscores[\"QWorkload\"][\"1s\"],\n",
    "                                    \"QWorkload_2\": cscores[\"QWorkload\"][\"2s\"],\n",
    "                                    \"QWorkload_3\": cscores[\"QWorkload\"][\"3s\"],\n",
    "                                    \"QWorkload_4\": cscores[\"QWorkload\"][\"4s\"],\n",
    "                                    \"QWorkload_5\": cscores[\"QWorkload\"][\"5s\"],})\n",
    "\n",
    "            # If the instructor scores exist...\n",
    "            if iscores != None:\n",
    "\n",
    "                # Retrieves the course name and separates it into department and number.\n",
    "                course = str(iscores[\"number\"]).split()\n",
    "                if course[0] == \"None\":\n",
    "                    course = [None, None]\n",
    "                    fullname = \"NODEPT-NOCOURSENUMBER\"\n",
    "                else:\n",
    "                    fullname = course[0] + \"-\" + course[1]\n",
    "\n",
    "                # Updates the dictionary with the data.\n",
    "                semdatadict.update({\"C_Department\": course[0],\n",
    "                                    \"C_Number\": course[1],\n",
    "                                    \"Course\": fullname,\n",
    "                                    \"C_CatNum\": iscores[\"cat_num\"],\n",
    "                                    \"C_ID\": iscores[\"course_id\"],\n",
    "                                    \"C_Year\": iscores[\"year\"],\n",
    "                                    \"C_Term\": iscores[\"term\"],\n",
    "                                    \"I_First\": iscores[\"first\"].strip(),\n",
    "                                    \"I_Last\": iscores[\"last\"].strip(),\n",
    "                                    \"I_ID\": iscores[\"id\"],\n",
    "                                    \"I_Overall\": iscores[\"InstructorOverall\"],\n",
    "                                    \"I_EffectiveLectures\": iscores[\"EffectiveLecturesorPresentations\"],\n",
    "                                    \"I_Accessible\": iscores[\"AccessibleOutsideClass\"],\n",
    "                                    \"I_GeneratesEnthusiasm\": iscores[\"GeneratesEnthusiasm\"],\n",
    "                                    \"I_EncouragesParticipation\": iscores[\"FacilitatesDiscussionEncouragesParticipation\"],\n",
    "                                    \"I_UsefulFeedback\": iscores[\"GivesUsefulFeedback\"],\n",
    "                                    \"I_ReturnsAssignmentsTimely\": iscores[\"ReturnsAssignmentsinTimelyFashion\"]})\n",
    "\n",
    "            # Adds the dictionary as a row to the dataframe.\n",
    "            bigdf = bigdf.append(semdatadict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a look at the resulting dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_Department</th>\n",
       "      <th>C_Number</th>\n",
       "      <th>Course</th>\n",
       "      <th>C_CatNum</th>\n",
       "      <th>C_ID</th>\n",
       "      <th>C_Semester</th>\n",
       "      <th>C_Year</th>\n",
       "      <th>C_Term</th>\n",
       "      <th>C_Overall</th>\n",
       "      <th>C_Workload</th>\n",
       "      <th>C_Difficulty</th>\n",
       "      <th>C_Recommendation</th>\n",
       "      <th>C_Enrollment</th>\n",
       "      <th>C_ResponseRate</th>\n",
       "      <th>I_First</th>\n",
       "      <th>I_Last</th>\n",
       "      <th>I_ID</th>\n",
       "      <th>I_Overall</th>\n",
       "      <th>I_EffectiveLectures</th>\n",
       "      <th>I_Accessible</th>\n",
       "      <th>I_GeneratesEnthusiasm</th>\n",
       "      <th>I_EncouragesParticipation</th>\n",
       "      <th>I_UsefulFeedback</th>\n",
       "      <th>I_ReturnsAssignmentsTimely</th>\n",
       "      <th>QOverall_1</th>\n",
       "      <th>QOverall_2</th>\n",
       "      <th>QOverall_3</th>\n",
       "      <th>QOverall_4</th>\n",
       "      <th>QOverall_5</th>\n",
       "      <th>QDifficulty_1</th>\n",
       "      <th>QDifficulty_2</th>\n",
       "      <th>QDifficulty_3</th>\n",
       "      <th>QDifficulty_4</th>\n",
       "      <th>QDifficulty_5</th>\n",
       "      <th>QWorkload_1</th>\n",
       "      <th>QWorkload_2</th>\n",
       "      <th>QWorkload_3</th>\n",
       "      <th>QWorkload_4</th>\n",
       "      <th>QWorkload_5</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTSCI</td>\n",
       "      <td>270</td>\n",
       "      <td>HISTSCI-270</td>\n",
       "      <td>58523</td>\n",
       "      <td>2697</td>\n",
       "      <td>Spring '12</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>4.67</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6</td>\n",
       "      <td>50.00</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>Lemov</td>\n",
       "      <td>79de794d3e2e19eb71a2033b0ec0b76d</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.33</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[This course is a perfect example of what grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '14</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>4.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>None</td>\n",
       "      <td>3.50</td>\n",
       "      <td>13</td>\n",
       "      <td>76.92</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.60</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[The class has a fairly high work load, but Dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '13</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>13</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Philosophy of the State with Dr. Chen offers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '12</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.47</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.47</td>\n",
       "      <td>15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.18</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[This was by far my favorite course. Dr. Chen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '11</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.62</td>\n",
       "      <td>13</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.92</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Be prepared to read, Discussions were great, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  C_Department C_Number        Course C_CatNum  C_ID  C_Semester C_Year C_Term C_Overall C_Workload C_Difficulty C_Recommendation C_Enrollment C_ResponseRate  I_First I_Last                              I_ID I_Overall I_EffectiveLectures I_Accessible I_GeneratesEnthusiasm I_EncouragesParticipation I_UsefulFeedback I_ReturnsAssignmentsTimely QOverall_1 QOverall_2 QOverall_3 QOverall_4 QOverall_5 QDifficulty_1 QDifficulty_2 QDifficulty_3 QDifficulty_4 QDifficulty_5 QWorkload_1 QWorkload_2  \\\n",
       "0      HISTSCI      270   HISTSCI-270    58523  2697  Spring '12   2011      2      4.67       2.33         3.33             5.00            6          50.00  Rebecca  Lemov  79de794d3e2e19eb71a2033b0ec0b76d      4.67                4.33         4.00                  4.33                      5.00             4.50                       4.00          0          0          0          1          2             0             0             2             1             0           0           2   \n",
       "1        EXPOS   20.132  EXPOS-20.132    22108  1676    Fall '14   2014      1      4.10       7.10         None             3.50           13          76.92     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1      4.50                4.60         3.70                  3.90                      3.90             4.10                       4.60          0          1          1          4          4             0             1             2             7             3           1           3   \n",
       "2        EXPOS   20.132  EXPOS-20.132    22108  1676    Fall '13   2013      1      3.50       2.60         3.90             3.20           13         100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1      3.80                4.00         2.50                  3.50                      4.10             4.30                       4.40          0          1          1          4          4             0             1             2             7             3           1           3   \n",
       "3        EXPOS   20.132  EXPOS-20.132    22108  1676    Fall '12   2012      1      3.73       2.47         3.67             3.47           15         100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1      3.87                4.33         2.64                  3.93                      4.18             3.64                       3.82          0          1          1          4          4             0             1             2             7             3           1           3   \n",
       "4        EXPOS   20.132  EXPOS-20.132    22108  1676    Fall '11   2011      1      3.85       2.00         3.54             3.62           13         100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1      4.08                3.75         3.31                  3.92                      4.46             4.23                       4.08          0          1          1          4          4             0             1             2             7             3           1           3   \n",
       "\n",
       "  QWorkload_3 QWorkload_4 QWorkload_5                                           Comments  \n",
       "0           1           0           0  [This course is a perfect example of what grea...  \n",
       "1           4           1           0  [The class has a fairly high work load, but Dr...  \n",
       "2           4           1           0  [Philosophy of the State with Dr. Chen offers ...  \n",
       "3           4           1           0  [This was by far my favorite course. Dr. Chen ...  \n",
       "4           4           1           0  [Be prepared to read, Discussions were great, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the instructor data was missing for a course, `C_Year` and `C_Term` were left out. We iterated over each row to extract this information from `C_semester`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterates over each row in the dataframe\n",
    "for index, row in bigdf.iterrows():\n",
    "\n",
    "    # Retrieves the year.\n",
    "    year = int(row[\"C_Semester\"][-2:])\n",
    "\n",
    "    # Based on semester, determines year and term.\n",
    "    if row[\"C_Semester\"][0:4] == \"Fall\":\n",
    "        row[\"C_Term\"] = 1\n",
    "        row[\"C_Year\"] = 2000 + year\n",
    "    elif row[\"C_Semester\"][0:6] == \"Spring\":\n",
    "        row[\"C_Term\"] = 2\n",
    "        row[\"C_Year\"] = 1999 + year\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to delete course that has no department or number. Although there is textual data in the comments, the missing number is a result of having no course and instructor scores, so there is little we can do with it numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigdf = bigdf[bigdf[\"C_Department\"]==bigdf[\"C_Department\"]]\n",
    "bigdf = bigdf[bigdf[\"C_Department\"]!=None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we adjusted the types of each column to prepare for data analysis. We created a dictionary `coltypes` with keys as types and values as a list of the columns that should be those types. All columns except `Comments` were categorized as either an `int`, a `float`, or a `string`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coltypes = {int: [\"C_CatNum\",\"C_ID\",\"C_Year\",\"C_Term\",\"C_Enrollment\",\n",
    "                  \"QOverall_1\",\"QOverall_2\",\"QOverall_3\",\"QOverall_4\",\"QOverall_5\",\n",
    "                  \"QDifficulty_1\",\"QDifficulty_2\",\"QDifficulty_3\",\"QDifficulty_4\",\"QDifficulty_5\",\n",
    "                  \"QWorkload_1\",\"QWorkload_2\",\"QWorkload_3\",\"QWorkload_4\",\"QWorkload_5\"],\n",
    "            float: [\"C_Overall\",\"C_Workload\",\"C_Difficulty\",\"C_Recommendation\",\"C_ResponseRate\",\n",
    "                    \"I_Overall\",\"I_EffectiveLectures\",\"I_Accessible\",\"I_GeneratesEnthusiasm\",\n",
    "                    \"I_EncouragesParticipation\",\"I_UsefulFeedback\",\"I_ReturnsAssignmentsTimely\",\n",
    "                    \"QOverall_1\",\"QOverall_2\",\"QOverall_3\",\"QOverall_4\",\"QOverall_5\"],\n",
    "            str: [\"C_Number\",\"C_Department\",\"C_Semester\",\"I_First\",\"I_Last\",\"I_ID\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the `string` cells contained unicode that could not be written to a csv. Using the `unidecode` library, we converted these to writable strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "for index, row in bigdf.iterrows():\n",
    "    for strcol in coltypes[str]:\n",
    "        try:\n",
    "            str(row[strcol])\n",
    "        except UnicodeEncodeError:\n",
    "            bigdf.loc[index, row[row.values==row[strcol]].index[0]] = unidecode(row[strcol])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we replaced `None` values with `NaN` for floats and `-1` for ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for intcol in coltypes[int]:\n",
    "    bigdf = bigdf.replace({intcol: {None: -1, \"\": -1}})\n",
    "\n",
    "for floatcol in coltypes[float]:\n",
    "    bigdf = bigdf.replace({floatcol: {None: np.nan}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish correcting the types, we cast the entire columns as the desired type using the keys in `coltypes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ctp in coltypes:\n",
    "    for col in coltypes[ctp]:\n",
    "        bigdf[col] = bigdf[col].astype(ctp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Adding an Indicator for Positive Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to provide some useful insight on what makes a good class, we had to determine how to classify courses as positive or negative. Ultimately, we decided that the threshold should be the median of overall q scores in the given semester, so that half of the courses would be good and half would be bad. We grouped by semester in order to reduce the effect of semester-specific events that could possibly shift that semester's scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adds an empty series for the semester median to the dataframe.\n",
    "bigdf[\"Sem_Median\"] = pd.Series(index=bigdf.index)\n",
    "\n",
    "# Iterates over each semester, adding the semester median to the series.\n",
    "for name, group in bigdf.groupby([\"C_Year\", \"C_Term\"]):\n",
    "    bigdf.loc[(bigdf[\"C_Year\"]==name[0])&(bigdf[\"C_Term\"]==name[1]), \"Sem_Median\"] = np.nanmedian(group[\"C_Overall\"].values)\n",
    "\n",
    "# Determines whether or not a review is positive by comparing the Q score to the semester median.\n",
    "bigdf[\"Positive\"] = pd.Series(bigdf[\"C_Overall\"]>=bigdf[\"Sem_Median\"], index=bigdf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a look at the final data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_Department</th>\n",
       "      <th>C_Number</th>\n",
       "      <th>Course</th>\n",
       "      <th>C_CatNum</th>\n",
       "      <th>C_ID</th>\n",
       "      <th>C_Semester</th>\n",
       "      <th>C_Year</th>\n",
       "      <th>C_Term</th>\n",
       "      <th>C_Overall</th>\n",
       "      <th>C_Workload</th>\n",
       "      <th>C_Difficulty</th>\n",
       "      <th>C_Recommendation</th>\n",
       "      <th>C_Enrollment</th>\n",
       "      <th>C_ResponseRate</th>\n",
       "      <th>I_First</th>\n",
       "      <th>I_Last</th>\n",
       "      <th>I_ID</th>\n",
       "      <th>I_Overall</th>\n",
       "      <th>I_EffectiveLectures</th>\n",
       "      <th>I_Accessible</th>\n",
       "      <th>I_GeneratesEnthusiasm</th>\n",
       "      <th>I_EncouragesParticipation</th>\n",
       "      <th>I_UsefulFeedback</th>\n",
       "      <th>I_ReturnsAssignmentsTimely</th>\n",
       "      <th>QOverall_1</th>\n",
       "      <th>QOverall_2</th>\n",
       "      <th>QOverall_3</th>\n",
       "      <th>QOverall_4</th>\n",
       "      <th>QOverall_5</th>\n",
       "      <th>QDifficulty_1</th>\n",
       "      <th>QDifficulty_2</th>\n",
       "      <th>QDifficulty_3</th>\n",
       "      <th>QDifficulty_4</th>\n",
       "      <th>QDifficulty_5</th>\n",
       "      <th>QWorkload_1</th>\n",
       "      <th>QWorkload_2</th>\n",
       "      <th>QWorkload_3</th>\n",
       "      <th>QWorkload_4</th>\n",
       "      <th>QWorkload_5</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sem_Median</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTSCI</td>\n",
       "      <td>270</td>\n",
       "      <td>HISTSCI-270</td>\n",
       "      <td>58523</td>\n",
       "      <td>2697</td>\n",
       "      <td>Spring '12</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>4.67</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6</td>\n",
       "      <td>50.00</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>Lemov</td>\n",
       "      <td>79de794d3e2e19eb71a2033b0ec0b76d</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.33</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[This course is a perfect example of what grea...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '14</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>4.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.50</td>\n",
       "      <td>13</td>\n",
       "      <td>76.92</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.60</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[The class has a fairly high work load, but Dr...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '13</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>13</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Philosophy of the State with Dr. Chen offers ...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '12</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.47</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.47</td>\n",
       "      <td>15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.18</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[This was by far my favorite course. Dr. Chen ...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '11</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.62</td>\n",
       "      <td>13</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.92</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Be prepared to read, Discussions were great, ...</td>\n",
       "      <td>4.20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  C_Department C_Number        Course  C_CatNum  C_ID  C_Semester  C_Year  C_Term  C_Overall  C_Workload  C_Difficulty  C_Recommendation  C_Enrollment  C_ResponseRate  I_First I_Last                              I_ID  I_Overall  I_EffectiveLectures  I_Accessible  I_GeneratesEnthusiasm  I_EncouragesParticipation  I_UsefulFeedback  I_ReturnsAssignmentsTimely  QOverall_1  QOverall_2  QOverall_3  QOverall_4  QOverall_5  QDifficulty_1  QDifficulty_2  QDifficulty_3  QDifficulty_4  QDifficulty_5  \\\n",
       "0      HISTSCI      270   HISTSCI-270     58523  2697  Spring '12    2011       2       4.67        2.33          3.33              5.00             6           50.00  Rebecca  Lemov  79de794d3e2e19eb71a2033b0ec0b76d       4.67                 4.33          4.00                   4.33                       5.00              4.50                        4.00           0           0           0           1           2              0              0              2              1              0   \n",
       "1        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '14    2014       1       4.10        7.10           NaN              3.50            13           76.92     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       4.50                 4.60          3.70                   3.90                       3.90              4.10                        4.60           0           1           1           4           4              0              1              2              7              3   \n",
       "2        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '13    2013       1       3.50        2.60          3.90              3.20            13          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       3.80                 4.00          2.50                   3.50                       4.10              4.30                        4.40           0           1           1           4           4              0              1              2              7              3   \n",
       "3        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '12    2012       1       3.73        2.47          3.67              3.47            15          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       3.87                 4.33          2.64                   3.93                       4.18              3.64                        3.82           0           1           1           4           4              0              1              2              7              3   \n",
       "4        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '11    2011       1       3.85        2.00          3.54              3.62            13          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       4.08                 3.75          3.31                   3.92                       4.46              4.23                        4.08           0           1           1           4           4              0              1              2              7              3   \n",
       "\n",
       "   QWorkload_1  QWorkload_2  QWorkload_3  QWorkload_4  QWorkload_5                                           Comments  Sem_Median Positive  \n",
       "0            0            2            1            0            0  [This course is a perfect example of what grea...        4.30     True  \n",
       "1            1            3            4            1            0  [The class has a fairly high work load, but Dr...        4.30    False  \n",
       "2            1            3            4            1            0  [Philosophy of the State with Dr. Chen offers ...        4.30    False  \n",
       "3            1            3            4            1            0  [This was by far my favorite course. Dr. Chen ...        4.25    False  \n",
       "4            1            3            4            1            0  [Be prepared to read, Discussions were great, ...        4.20    False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write this to a csv file for safekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigdf.to_csv(\"bigdf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Numerical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Statistics and Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we plotted the distribution of overall course scores by semester as a part of our exploratory analysis. We can observe that there is a high density around the `4-5` range, with a mean of `4.18` and a median of `4.25`, indicating that the histogram is skewed left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot();\n",
    "\n",
    "# Plots a histogram of each semester's overall course scores.\n",
    "for name, group in bigdf.groupby([\"C_Year\", \"C_Term\"]):\n",
    "    group[\"C_Overall\"].hist(bins=50, alpha=0.1);\n",
    "\n",
    "# Calculates the mean and median.\n",
    "mean = bigdf[\"C_Overall\"].mean()\n",
    "median = bigdf[\"C_Overall\"].median()\n",
    "\n",
    "# Adds the mean and median to the histogram, with a legend.\n",
    "plt.axvline(mean, color=\"green\", label=\"Mean: %0.2f\" % mean, alpha=0.5);\n",
    "plt.axvline(median, color =\"red\", label=\"Median: %0.2f\" % median, alpha=0.5);\n",
    "plt.legend(loc=\"upper left\");\n",
    "\n",
    "# Formats the plot.\n",
    "plt.xlim(1,5);\n",
    "plt.xlabel(\"Overall Course Score\");\n",
    "plt.ylabel(\"Frequency\");\n",
    "plt.title(\"Distribution of Overall Course Scores, by Semester\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the highest and lowest rated classes.\n",
    "\n",
    "using `groupby`. The CHEMBIO department has the lowest average course scores at about `3.12`, and the PLSH, MEDGREEK, PAL, and GIKUYU departments have had perfect scores for each course in Q guide history. The perfect scores are likely due to low enrollment and people who self-select into those course offerings, whereas the low scores are coming from difficult science departments that could offering requirements for the premed track or other rigorous curricula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ASDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the highest rated departments and the lowest, using `groupby`. The CHEMBIO department has the lowest average course scores at about `3.12`, and the PLSH, MEDGREEK, PAL, and GIKUYU departments have had perfect scores for each course in Q guide history. The perfect scores are likely due to low enrollment and people who self-select into those course offerings, whereas the low scores are coming from difficult science departments that could offering requirements for the premed track or other rigorous curricula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a series of department mean overall course scores.\n",
    "deprank = bigdf.groupby(\"C_Department\")[\"C_Overall\"].mean()\n",
    "\n",
    "# Prints the top 5 and bottom 5 departments.\n",
    "deprank.sort(ascending=False)\n",
    "print \"TOP 5 DEPARTMENTS:\\n\", deprank.head(5), \"\\n\"\n",
    "deprank.sort(ascending=True)\n",
    "print \"BOTTOM 5 DEPARTMENTS:\\n\", deprank.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting trend is the slight discrepancy between Fall scores and Spring scores. In the entire history of the Q, Fall score means have been consistently lower than those in the Spring. However, both have risen over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a series of semester mean overall course scores.\n",
    "semrank = bigdf.groupby(\"C_Semester\")[\"C_Overall\"].mean()\n",
    "\n",
    "# Plots by semester.\n",
    "semrank[0:4].plot(label=\"Fall\")\n",
    "semrank[5:9].plot(label=\"Spring\")\n",
    "\n",
    "# Formats and labels the plot.\n",
    "plt.xticks(range(4),['2011','2012','2013','2014']);\n",
    "plt.ylim(1,5)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.xlabel(\"Mean of Overall Course Scores\")\n",
    "plt.title(\"Mean of Overall Course Scores vs. Year\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to figure out which numerical data are most responsible for the overall course score, we created a copy of the dataframe specifically for numerical analysis, then removed certain columns that we should avoid running regressions on, such as the breakdown of ratings (how many 1s, 2s, etc.), the textual reviews, any string fields, and the course catalog number and id; for the sake of easily identifying courses, we kept department, number, and semester. We removed all reviews from `Fall '14`, as the format of the Q guide changed that semester. Finally, we dropped all rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copies the big dataframe.\n",
    "numdf = bigdf.copy()\n",
    "\n",
    "# Drops columns that we will not be needing.\n",
    "numdf = numdf.drop(numdf.columns[[2,3,4,6,7,11]+list(range(13,17))+list(range(24,40))], axis=1)\n",
    "\n",
    "# Removes Fall '14 data.\n",
    "numdf = numdf[numdf[\"C_Semester\"] != \"Fall '14\"]\n",
    "\n",
    "# Drops rows with missing data.\n",
    "numdf = numdf.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the resulting dataframe for our numerical analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into training and test sets. Since we are trying to use previous Q data to predict next semester's, we tested on `Spring '14` data and trained on the data in previous semesters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separates Spring '14 into the test set and the rest into the training set.\n",
    "train = numdf[numdf[\"C_Semester\"] != \"Spring '14\"]\n",
    "test = numdf[numdf[\"C_Semester\"] == \"Spring '14\"]\n",
    "\n",
    "# Prints the size of the sets.\n",
    "print \"Length of Train is:\", len(train)\n",
    "print \"Length of Test is: \", len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we included non-numerical columns in the dataframe, we created a list `lcols` to specify which columns should be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a list of all the columns we have in the numerical dataframe.\n",
    "lcols = list(train.columns)\n",
    "\n",
    "# Removes columns we do not need for training.\n",
    "for c in [\"C_Department\",\"C_Number\",\"C_Semester\",\"C_Overall\",\"Sem_Median\",\"Positive\"]:\n",
    "    lcols.remove(c)\n",
    "\n",
    "# Shows the remaining columns.\n",
    "lcols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will separate the response variable, `Positive`, from the data in `lcols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the X matrix.\n",
    "Xtrain = train[lcols].values\n",
    "Xtest = test[lcols].values\n",
    "\n",
    "# Creates the y response vector.\n",
    "ytrain = train[\"Positive\"].values\n",
    "ytest = test[\"Positive\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.21 Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in homework 3, we will start off with a Linear SVM because of its quickness, running `GridSearchCV` over a range of regularization coefficients with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Creates a Linear SVM to be trained.\n",
    "clfsvm = LinearSVC(loss=\"hinge\")\n",
    "\n",
    "# Creates a list of regularization coefficients to be used.\n",
    "Cs = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Performs a grid serach on the Linear SVM\n",
    "gs = GridSearchCV(clfsvm, param_grid={'C':Cs}, cv=5)\n",
    "gs.fit(Xtrain, ytrain)\n",
    "\n",
    "# Prints the best C value and its accuracy.\n",
    "print \"The best value of C is: %0.3f\" % gs.best_params_[\"C\"]\n",
    "print \"The accuracy is:        %0.4f\" % gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will refit the data again using the best classifier given by the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stores the best classifier.\n",
    "best = gs.best_estimator_\n",
    "\n",
    "# Refits the training data.\n",
    "best.fit(Xtrain, ytrain)\n",
    "\n",
    "# Prints the refitted accuracy.\n",
    "print \"Refitting the training data with C = %0.3f:\" % gs.best_params_[\"C\"]\n",
    "print \"Training accuracy is: %0.4f\" % best.score(Xtrain, ytrain)\n",
    "print \"Test accuracy is:     %0.4f\" % best.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.22 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will move onto a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "model = LogisticRegression()\n",
    "logisticmodel = model.fit(Xtrain, ytrain)\n",
    "\n",
    "# Prints the accuracies on both sets.\n",
    "print \"Training accuracy is:\", model.score(Xtrain, ytrain) \n",
    "print \"Test accuracy is:    \", model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn import ensemble\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rfmodel = rf.fit(Xtrain, ytrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = rfmodel.score(Xtest,ytest) \n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "\n",
    "    initial = False\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "        initial=True\n",
    "\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds = roc_curve(ytest, clf.decision_function(xtest))\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    if skip:\n",
    "        l = fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(boxstyle='round,pad=0.3', alpha=0.2)\n",
    "\n",
    "    if labe!=None:\n",
    "        for k in xrange(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.hls_palette(8, l=.3, s=.8):\n",
    "    ax = make_roc(\"logistic-regression\", model, ytest, Xtest, labe=200, skip=50) \n",
    "    ax = make_roc(\"random-forest-classifier\", rfmodel, ytest, Xtest, labe=20, skip=10) \n",
    "    ax = make_roc(\"linear-svm-all-features\", best, ytest, Xtest, labe=200, proba=False, skip=50)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nonzero_lasso(clf):\n",
    "    featuremask = (clf.coef_ !=0.0)[0]\n",
    "    return pd.DataFrame(dict(feature=lcols, coef=clf.coef_[0], abscoef=np.abs(clf.coef_[0])))[featuremask].sort('abscoef', ascending=False) \n",
    "\n",
    "lasso_importances = nonzero_lasso(best)\n",
    "lasso_importances.set_index(\"feature\", inplace=True)\n",
    "lasso_importances.reset_index(\"feature\", inplace=True)\n",
    "lasso_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's run a pipelined classifier with feature selection to see if we can improve our classifier.  \n",
    "from sklearn import feature_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pearson_scorer(X,y):\n",
    "    rs = np.zeros(X.shape[1])\n",
    "    pvals = np.zeros(X.shape[1])\n",
    "    i = 0\n",
    "    for v in X.T:\n",
    "        rs[i], pvals[i] = pearsonr(v, y)\n",
    "        i = i + 1\n",
    "    return np.abs(rs), pvals\n",
    "\n",
    "selectorlinearsvm = SelectKBest(k=10, score_func=pearson_scorer)\n",
    "pipelinearsvm = Pipeline([('select', selectorlinearsvm), ('svm', LinearSVC(loss=\"hinge\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can see if our pipeline linear SVM classifier with feature select did better than our original linear SVM classifier.\n",
    "pipeline = pipelinearsvm.fit(Xtrain, ytrain) \n",
    "pipeline.score(Xtest, ytest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#What features did the pipeline classifier select? \n",
    "np.array(lcols)[pipeline.get_params()[\"select\"].get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sns.hls_palette(8, l=.3, s=.8):\n",
    "    ax = make_roc(\"logistic-regression\",model, ytest, Xtest, labe=200, skip=50) \n",
    "    ax = make_roc(\"random-forest-classifier\", rfmodel, ytest, Xtest, labe=20, skip=10) \n",
    "    ax = make_roc(\"linear-svm-all-features\", best, ytest, Xtest, labe=200, proba=False, skip=10)   \n",
    "    ax = make_roc(\"linear-svm-feature-selected\", pipeline, ytest, Xtest, labe=200, proba=False, skip=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical part of any review are the comments. We will now proceed to analyze the comments in our q data. We will judge the predictive power of these comments, and analyze the role they play in a score's q rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from itertools import chain\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"unit\" that we are looking at is the course-semester. In the previous notebook, for each course-semester, we had all comments, all associated Q values, and an attribute 'Positive' which is true if the average Q score for that course for that semester is greater than or equal to the average Q score across all courses for that semester. The 'Positive' attribute is false otherwise. In other words, a course-semester is defined to have an overall positive rating (Positive = True) if its Q score is above the average Q score across all courses for that semester.\n",
    "\n",
    "We begin by loading in our complete dataframe of comments, which was stored in a csv from the last Python notebook. We will randomly subsample 10 comments per course-semester and only consider course-semesters with 10 or more comments. Our subsampled dataframe will consist of 5 columns: Course, C_Semester, Positive, C_Overall, and Sampled_Comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_PER_COURSESEM_REVIEWS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_Department</th>\n",
       "      <th>C_Number</th>\n",
       "      <th>Course</th>\n",
       "      <th>C_CatNum</th>\n",
       "      <th>C_ID</th>\n",
       "      <th>C_Semester</th>\n",
       "      <th>C_Year</th>\n",
       "      <th>C_Term</th>\n",
       "      <th>C_Overall</th>\n",
       "      <th>C_Workload</th>\n",
       "      <th>C_Difficulty</th>\n",
       "      <th>C_Recommendation</th>\n",
       "      <th>C_Enrollment</th>\n",
       "      <th>C_ResponseRate</th>\n",
       "      <th>I_First</th>\n",
       "      <th>I_Last</th>\n",
       "      <th>I_ID</th>\n",
       "      <th>I_Overall</th>\n",
       "      <th>I_EffectiveLectures</th>\n",
       "      <th>I_Accessible</th>\n",
       "      <th>I_GeneratesEnthusiasm</th>\n",
       "      <th>I_EncouragesParticipation</th>\n",
       "      <th>I_UsefulFeedback</th>\n",
       "      <th>I_ReturnsAssignmentsTimely</th>\n",
       "      <th>QOverall_1</th>\n",
       "      <th>QOverall_2</th>\n",
       "      <th>QOverall_3</th>\n",
       "      <th>QOverall_4</th>\n",
       "      <th>QOverall_5</th>\n",
       "      <th>QDifficulty_1</th>\n",
       "      <th>QDifficulty_2</th>\n",
       "      <th>QDifficulty_3</th>\n",
       "      <th>QDifficulty_4</th>\n",
       "      <th>QDifficulty_5</th>\n",
       "      <th>QWorkload_1</th>\n",
       "      <th>QWorkload_2</th>\n",
       "      <th>QWorkload_3</th>\n",
       "      <th>QWorkload_4</th>\n",
       "      <th>QWorkload_5</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Sem_Median</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTSCI</td>\n",
       "      <td>270</td>\n",
       "      <td>HISTSCI-270</td>\n",
       "      <td>58523</td>\n",
       "      <td>2697</td>\n",
       "      <td>Spring '12</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>4.67</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6</td>\n",
       "      <td>50.00</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>Lemov</td>\n",
       "      <td>79de794d3e2e19eb71a2033b0ec0b76d</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.33</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'This course is a perfect example of what gr...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '14</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>4.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.50</td>\n",
       "      <td>13</td>\n",
       "      <td>76.92</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.60</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'The class has a fairly high work load, but ...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '13</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.20</td>\n",
       "      <td>13</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'Philosophy of the State with Dr. Chen offer...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '12</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.47</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.47</td>\n",
       "      <td>15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.18</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'This was by far my favorite course. Dr. Che...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXPOS</td>\n",
       "      <td>20.132</td>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>22108</td>\n",
       "      <td>1676</td>\n",
       "      <td>Fall '11</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.62</td>\n",
       "      <td>13</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Chen</td>\n",
       "      <td>1341ccb7bd27f47e68625b63b15281d1</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.92</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'Be prepared to read', u'Discussions were gr...</td>\n",
       "      <td>4.20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  C_Department C_Number        Course  C_CatNum  C_ID  C_Semester  C_Year  C_Term  C_Overall  C_Workload  C_Difficulty  C_Recommendation  C_Enrollment  C_ResponseRate  I_First I_Last                              I_ID  I_Overall  I_EffectiveLectures  I_Accessible  I_GeneratesEnthusiasm  I_EncouragesParticipation  I_UsefulFeedback  I_ReturnsAssignmentsTimely  QOverall_1  QOverall_2  QOverall_3  QOverall_4  QOverall_5  QDifficulty_1  QDifficulty_2  QDifficulty_3  QDifficulty_4  QDifficulty_5  \\\n",
       "0      HISTSCI      270   HISTSCI-270     58523  2697  Spring '12    2011       2       4.67        2.33          3.33              5.00             6           50.00  Rebecca  Lemov  79de794d3e2e19eb71a2033b0ec0b76d       4.67                 4.33          4.00                   4.33                       5.00              4.50                        4.00           0           0           0           1           2              0              0              2              1              0   \n",
       "1        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '14    2014       1       4.10        7.10           NaN              3.50            13           76.92     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       4.50                 4.60          3.70                   3.90                       3.90              4.10                        4.60           0           1           1           4           4              0              1              2              7              3   \n",
       "2        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '13    2013       1       3.50        2.60          3.90              3.20            13          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       3.80                 4.00          2.50                   3.50                       4.10              4.30                        4.40           0           1           1           4           4              0              1              2              7              3   \n",
       "3        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '12    2012       1       3.73        2.47          3.67              3.47            15          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       3.87                 4.33          2.64                   3.93                       4.18              3.64                        3.82           0           1           1           4           4              0              1              2              7              3   \n",
       "4        EXPOS   20.132  EXPOS-20.132     22108  1676    Fall '11    2011       1       3.85        2.00          3.54              3.62            13          100.00     Owen   Chen  1341ccb7bd27f47e68625b63b15281d1       4.08                 3.75          3.31                   3.92                       4.46              4.23                        4.08           0           1           1           4           4              0              1              2              7              3   \n",
       "\n",
       "   QWorkload_1  QWorkload_2  QWorkload_3  QWorkload_4  QWorkload_5                                           Comments  Sem_Median Positive  \n",
       "0            0            2            1            0            0  [u'This course is a perfect example of what gr...        4.30     True  \n",
       "1            1            3            4            1            0  [u'The class has a fairly high work load, but ...        4.30    False  \n",
       "2            1            3            4            1            0  [u'Philosophy of the State with Dr. Chen offer...        4.30    False  \n",
       "3            1            3            4            1            0  [u'This was by far my favorite course. Dr. Che...        4.25    False  \n",
       "4            1            3            4            1            0  [u'Be prepared to read', u'Discussions were gr...        4.20    False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf=pd.read_csv(\"bigdf.csv\")\n",
    "bigdf.reset_index(drop=True)\n",
    "bigdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a helper function sample_comments that takes in a list of comments in string form and then returns one string that contains MIN_PER_COURSEM_REVIEWS randomly selected comments concatenated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_comments(commentsListAsString):\n",
    "    if type(commentsListAsString) != str:\n",
    "        return \"\"\n",
    "    else:\n",
    "        allComments = ast.literal_eval(commentsListAsString)\n",
    "        if len(allComments) >= MIN_PER_COURSESEM_REVIEWS:\n",
    "            return \" \".join(np.random.choice(allComments, MIN_PER_COURSESEM_REVIEWS, replace=False))\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "subdf = bigdf[[\"Course\",\"C_Semester\",\"Comments\",\"Positive\",\"C_Overall\"]].dropna()\n",
    "subdf[\"Sampled_Comments\"] = subdf.Comments.map(sample_comments)\n",
    "subdf = subdf[subdf.Sampled_Comments != \"\"]\n",
    "subdf[\"Course_Semester\"] = subdf.Course + \"-\" + subdf.C_Semester\n",
    "subdf = subdf[[\"Course\",\"C_Semester\",\"Course_Semester\",\"Positive\",\"C_Overall\",\"Sampled_Comments\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a look at the textual dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Course</th>\n",
       "      <th>C_Semester</th>\n",
       "      <th>Course_Semester</th>\n",
       "      <th>Positive</th>\n",
       "      <th>C_Overall</th>\n",
       "      <th>Sampled_Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>Fall '13</td>\n",
       "      <td>EXPOS-20.132-Fall '13</td>\n",
       "      <td>False</td>\n",
       "      <td>3.50</td>\n",
       "      <td>I would inform them that this class demands ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXPOS-20.132</td>\n",
       "      <td>Fall '12</td>\n",
       "      <td>EXPOS-20.132-Fall '12</td>\n",
       "      <td>False</td>\n",
       "      <td>3.73</td>\n",
       "      <td>This was by far my favorite course. Dr. Chen i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EXPOS-20.131</td>\n",
       "      <td>Fall '14</td>\n",
       "      <td>EXPOS-20.131-Fall '14</td>\n",
       "      <td>False</td>\n",
       "      <td>3.80</td>\n",
       "      <td>- perhaps a bit intimidating, this is an enjoy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EXPOS-20.131</td>\n",
       "      <td>Fall '13</td>\n",
       "      <td>EXPOS-20.131-Fall '13</td>\n",
       "      <td>False</td>\n",
       "      <td>3.60</td>\n",
       "      <td>Do not take this course. The readings are long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EXPOS-20.131</td>\n",
       "      <td>Fall '11</td>\n",
       "      <td>EXPOS-20.131-Fall '11</td>\n",
       "      <td>True</td>\n",
       "      <td>4.50</td>\n",
       "      <td>If you are interested in political philosophy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Course C_Semester        Course_Semester Positive  C_Overall                                   Sampled_Comments\n",
       "2   EXPOS-20.132   Fall '13  EXPOS-20.132-Fall '13    False       3.50  I would inform them that this class demands ma...\n",
       "3   EXPOS-20.132   Fall '12  EXPOS-20.132-Fall '12    False       3.73  This was by far my favorite course. Dr. Chen i...\n",
       "13  EXPOS-20.131   Fall '14  EXPOS-20.131-Fall '14    False       3.80  - perhaps a bit intimidating, this is an enjoy...\n",
       "14  EXPOS-20.131   Fall '13  EXPOS-20.131-Fall '13    False       3.60  Do not take this course. The readings are long...\n",
       "16  EXPOS-20.131   Fall '11  EXPOS-20.131-Fall '11     True       4.50  If you are interested in political philosophy ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our comments dataframe, subdf, to a spark dataframe for text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec\n"
     ]
    }
   ],
   "source": [
    "#setup spark\n",
    "import os\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "\n",
    "conf = (pyspark.SparkConf()\n",
    "        .setMaster('local')\n",
    "        .setAppName('pyspark')\n",
    "        .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "import sys\n",
    "rdd = sc.parallelize(xrange(10), 10)\n",
    "rdd.map(lambda x: sys.version).collect()\n",
    "sys.version\n",
    "\n",
    "sqlsc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "regex1 = re.compile(r\"\\.{2,}\")\n",
    "regex2 = re.compile(r\"\\-{2,}\")\n",
    "\n",
    "#Useless verbs courtesy of: http://mbweston.com/2012/11/26/writing-editing-find-and-eliminate-useless-verbs/\n",
    "uselessverbs = ['be','is','are','be','was','were','been','being',\n",
    "                'go','goes','went','gone','going','put','puts','putting',\n",
    "                'do','does','did','done','doing',\n",
    "                'come','comes','came','coming',\n",
    "                'have','have','has','had','having',\n",
    "                'can','could','begin','begins','began','begun','beginning',\n",
    "                'seem','seems','seemed','seeming',\n",
    "                'get','got','gotten','getting',\n",
    "                'become','became','becoming']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a get parts function to parse the language in the comments. This customized get_parts function returns lists of the nouns, adjectives and verbs in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parts(thetext):\n",
    "    thetext = re.sub(regex1, ' ', thetext)\n",
    "    thetext = re.sub(regex2, ' ', thetext)\n",
    "    nouns = []\n",
    "    descriptives = []\n",
    "    verbs = []\n",
    "    for i, sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        verbs.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) > 0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4]) == 1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4]) == 1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "                elif token[1] in ['VB','VBP','VBZ','VBG','VBD','VBN']:\n",
    "                    if token[4] in stopwords or token[4] in uselessverbs or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4]) == 1:\n",
    "                        continue\n",
    "                    verbs[i].append(token[4])\n",
    "    out = zip(nouns, descriptives, verbs)\n",
    "    nouns2 = []\n",
    "    descriptives2 = []\n",
    "    verbs2 = []\n",
    "    for n, d, v in out:\n",
    "        if len(n) != 0 and len(d) ! =0 and len(v) != 0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "            verbs2.append(v)\n",
    "    return nouns2, descriptives2, verbs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------+--------+---------+--------------------+\n",
      "|      Course|C_Semester|     Course_Semester|Positive|C_Overall|    Sampled_Comments|\n",
      "+------------+----------+--------------------+--------+---------+--------------------+\n",
      "|EXPOS-20.132|  Fall '13|EXPOS-20.132-Fall...|   false|      3.5|I would inform th...|\n",
      "|EXPOS-20.132|  Fall '12|EXPOS-20.132-Fall...|   false|     3.73|This was by far m...|\n",
      "|EXPOS-20.131|  Fall '14|EXPOS-20.131-Fall...|   false|      3.8|- perhaps a bit i...|\n",
      "|EXPOS-20.131|  Fall '13|EXPOS-20.131-Fall...|   false|      3.6|Do not take this ...|\n",
      "|EXPOS-20.131|  Fall '11|EXPOS-20.131-Fall...|    true|      4.5|If you are intere...|\n",
      "+------------+----------+--------------------+--------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subdf = sqlsc.createDataFrame(subdf)\n",
    "subdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect the parts of speech (nouns, adjectives, and verbs) for our randomly selected comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[u'philosophy',\n",
       "    u'state',\n",
       "    u'discussion',\n",
       "    u'struggle',\n",
       "    u'balance',\n",
       "    u'nature',\n",
       "    u'society',\n",
       "    u'course'],\n",
       "   [u'page',\n",
       "    u'reading',\n",
       "    u'day',\n",
       "    u'syllabus',\n",
       "    u'warning',\n",
       "    u'thing',\n",
       "    u'tardiness',\n",
       "    u'grade'],\n",
       "   [u'course'],\n",
       "   [u'reading', u'understanding', u'work'],\n",
       "   [u'class', u'lot'],\n",
       "   [u'class', u'improvement', u'writing', u'understanding', u'writing'],\n",
       "   [u'class'],\n",
       "   [u'lot', u'philosophy', u'professor', u'lot'],\n",
       "   [u'course', u'lot', u'reading', u'writing'],\n",
       "   [u'philosophy', u'state', u'class', u'writing', u'essay', u'college'],\n",
       "   [u'matter', u'society'],\n",
       "   [u'page', u'entirety', u'work'],\n",
       "   [u'class', u'material', u'experience'],\n",
       "   [u'student', u'class', u'philosophy', u'level', u'discussion'],\n",
       "   [u'class', u'philosophy', u'introduction', u'course', u'ton', u'student'],\n",
       "   [u'class', u'discussion', u'class', u'lot', u'philosophy'],\n",
       "   [u'state',\n",
       "    u'philosophy',\n",
       "    u'course',\n",
       "    u'philosophy',\n",
       "    u'stimulating',\n",
       "    u'class',\n",
       "    u'discussion',\n",
       "    u'variety',\n",
       "    u'topic',\n",
       "    u'student',\n",
       "    u'share']],\n",
       "  [[u'awesome'],\n",
       "   [u'dense', u'occasional', u'terrible'],\n",
       "   [u'interesting', u'intensive', u'difficult'],\n",
       "   [u'interesting',\n",
       "    u'mandatory',\n",
       "    u'solid',\n",
       "    u'important',\n",
       "    u'interesting',\n",
       "    u'philosophical'],\n",
       "   [u'great'],\n",
       "   [u'concrete', u'greater', u'effective'],\n",
       "   [u'difficult', u'accurate'],\n",
       "   [u'interesting', u'great'],\n",
       "   [u'critical'],\n",
       "   [u'difficult', u'better'],\n",
       "   [u'subject', u'interesting'],\n",
       "   [u'difficult', u'unnecessary'],\n",
       "   [u'potential', u'interesting'],\n",
       "   [u'high', u'engaging'],\n",
       "   [u'fellow'],\n",
       "   [u'interesting', u'willing', u'interesting'],\n",
       "   [u'passionate', u'wide', u'fellow', u'similar']],\n",
       "  [[u'say', u'enjoy'],\n",
       "   [u'assign', u'offer', u'consist', u'include', u'earn'],\n",
       "   [u'learn'],\n",
       "   [u'assign', u'develop'],\n",
       "   [u'read'],\n",
       "   [u'assume', u'invest', u'earn'],\n",
       "   [u'rumore'],\n",
       "   [u'learn', u'expect'],\n",
       "   [u'want', u'learn'],\n",
       "   [u'make'],\n",
       "   [u'make', u'think', u'live'],\n",
       "   [u'work', u'read', u'engage'],\n",
       "   [u'excite', u'think', u'make', u'disappoint'],\n",
       "   [u'love'],\n",
       "   [u'know', u'want', u'learn'],\n",
       "   [u'read', u'frustrate'],\n",
       "   [u'offer', u'cover']])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_parts = subdf.rdd.map(lambda r: get_parts(r.Sampled_Comments))\n",
    "comment_parts.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsedcomments = comment_parts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin our text analysis with an LDA of the nouns in the comments.\n",
    "\n",
    "The list of lists of nouns for the first course-semester represented in parsedcomments is shown below. Each individual list of nouns represents the nouns of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'philosophy',\n",
       "   u'state',\n",
       "   u'discussion',\n",
       "   u'struggle',\n",
       "   u'balance',\n",
       "   u'nature',\n",
       "   u'society',\n",
       "   u'course'],\n",
       "  [u'page',\n",
       "   u'reading',\n",
       "   u'day',\n",
       "   u'syllabus',\n",
       "   u'warning',\n",
       "   u'thing',\n",
       "   u'tardiness',\n",
       "   u'grade'],\n",
       "  [u'course'],\n",
       "  [u'reading', u'understanding', u'work'],\n",
       "  [u'class', u'lot'],\n",
       "  [u'class', u'improvement', u'writing', u'understanding', u'writing'],\n",
       "  [u'class'],\n",
       "  [u'lot', u'philosophy', u'professor', u'lot'],\n",
       "  [u'course', u'lot', u'reading', u'writing'],\n",
       "  [u'philosophy', u'state', u'class', u'writing', u'essay', u'college'],\n",
       "  [u'matter', u'society'],\n",
       "  [u'page', u'entirety', u'work'],\n",
       "  [u'class', u'material', u'experience'],\n",
       "  [u'student', u'class', u'philosophy', u'level', u'discussion'],\n",
       "  [u'class', u'philosophy', u'introduction', u'course', u'ton', u'student'],\n",
       "  [u'class', u'discussion', u'class', u'lot', u'philosophy'],\n",
       "  [u'state',\n",
       "   u'philosophy',\n",
       "   u'course',\n",
       "   u'philosophy',\n",
       "   u'stimulating',\n",
       "   u'class',\n",
       "   u'discussion',\n",
       "   u'variety',\n",
       "   u'topic',\n",
       "   u'student',\n",
       "   u'share']]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e[0] for e in parsedcomments[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flatten once so that the first five elements of ldadatardd are five sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'philosophy',\n",
       "  u'state',\n",
       "  u'discussion',\n",
       "  u'struggle',\n",
       "  u'balance',\n",
       "  u'nature',\n",
       "  u'society',\n",
       "  u'course'],\n",
       " [u'page',\n",
       "  u'reading',\n",
       "  u'day',\n",
       "  u'syllabus',\n",
       "  u'warning',\n",
       "  u'thing',\n",
       "  u'tardiness',\n",
       "  u'grade'],\n",
       " [u'course'],\n",
       " [u'reading', u'understanding', u'work'],\n",
       " [u'class', u'lot']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldadatardd = sc.parallelize([ele[0] for ele in parsedcomments]).flatMap(lambda l: l)\n",
    "ldadatardd.cache()\n",
    "ldadatardd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flatten once more so that the first five elements of ldadatardd are five words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'philosophy', u'state', u'discussion', u'struggle', u'balance']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldadatardd.flatMap(lambda word: word).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the vocabulary of the comments with and zipWithIndex in anticipation of making the corpus vector later where it will be useful to have a place index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabtups = (ldadatardd.flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the collectAsMap function to create vocab, a dictionary of vocabulary with words as keys and index as values.\n",
    "\n",
    "In similar fashion, we create id2word, a dictionary with index (id) as keys and vocab as words.\n",
    "The output below confirms that the relationships between id2word and vocab is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vocabtups.collectAsMap()\n",
    "id2word = vocabtups.map(lambda (x,y): (y,x)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'diaorganization', u'dynasty', 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0], vocab.keys()[5], vocab[vocab.keys()[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a unique vocabulary of 4375 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4456"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create documents, a list of tuple lists where each tuplelist represents a sentence.\n",
    "\n",
    "The first element of each tuple is the word id and the second element of the tuple is the number of times that the word appears in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_wordID_count_tuples(wordList):\n",
    "    d = defaultdict(int)\n",
    "    for k in wordList:\n",
    "        d[vocab[k]] += 1\n",
    "    return d.items()\n",
    "documents = ldadatardd.map(lambda w: get_wordID_count_tuples(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below structure for documents appears to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(3552, 1),\n",
       "  (2529, 1),\n",
       "  (3656, 1),\n",
       "  (398, 1),\n",
       "  (632, 1),\n",
       "  (1300, 1),\n",
       "  (1358, 1),\n",
       "  (1656, 1)],\n",
       " [(155, 1),\n",
       "  (4228, 1),\n",
       "  (3173, 1),\n",
       "  (3623, 1),\n",
       "  (1486, 1),\n",
       "  (1717, 1),\n",
       "  (3227, 1),\n",
       "  (413, 1)],\n",
       " [(1358, 1)],\n",
       " [(1185, 1), (1717, 1), (1149, 1)],\n",
       " [(2055, 1), (3839, 1)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = documents.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin our unsupervised LDA topic extraction on the nouns only (instead of verbs or adjectives), since intuitively, the nouns likely reflect the main themes and topics of sentences. We will run the LDA on documents of chunk size 20000 which is hopefully sufficient for the topics to converge. We set the num_topics parameter to 2 because we are looking for to separate our nouns into 2 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "lda2 = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=2, id2word=id2word, chunksize=20000, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we print the topics we find using LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.119*course + 0.038*lot + 0.035*material + 0.029*work + 0.024*time + 0.020*problem + 0.014*student + 0.013*semester + 0.012*assignment + 0.012*set',\n",
       " u'0.157*class + 0.031*lecture + 0.027*way + 0.021*reading + 0.018*lot + 0.017*fun + 0.017*professor + 0.016*time + 0.014*language + 0.013*student']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first topic (let us call this Topic 0) includes the combination of words:\n",
    "* class, lot, time, work, way, professor, problem, section, exam, and fun.\n",
    "\n",
    "The second topic (let us call this Topic 1) includes the following combination of words:\n",
    "* course, material, lecture, student, topic, reading, person, science, year, and lecturer.\n",
    "\n",
    "Topic 0 seems to encompass the more interactive, qualitative, personable aspects of the course with key terms including class, professor, problem, section, and fun. Topic 1, by contrast, seems to encompass the more solitary, logistical, factual aspects of a course that a student experiences with key terms including material, lecture, student, topic, reading, year, and lecturer. One thing worth noticing is that Topic 1 includes \"course\" as a key term while Topic 0 includes \"class\". While \"course\" and \"class\" are often used interchangably in language, \"class\" arguably connotes a more personal, interactive experience than \"course\", which is more administrative and logistical and more likely to be used as an umbrella term for everything from everyday class to homework. This would support our separation of Topic 0 and 1 into more interactive/personable aspects and more logistical/solitary aspects, respectively.\n",
    "\n",
    "In order to further evaluate our intial hypothesis that course reviews are split along two topics (interactive, qualitiative, personable aspects v. solitary, logistical aspects), we will output the words of some sentences, along with the probability of the sentence belonging to Topic 0 and Topic 1, to qualitatively check that our topics are reasonable and supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3552, 1), (2529, 1), (3656, 1), (398, 1), (632, 1), (1300, 1), (1358, 1), (1656, 1)]\n",
      "[(0, 0.54266383346916935), (1, 0.45733616653083065)]\n",
      "philosophy balance society struggle nature discussion course state\n",
      "==========================================\n",
      "[(3051, 1), (3839, 1)]\n",
      "[(0, 0.2191246771786691), (1, 0.78087532282133099)]\n",
      "bit class\n",
      "==========================================\n",
      "[(923, 1), (2299, 1)]\n",
      "[(0, 0.83177749867592754), (1, 0.16822250132407246)]\n",
      "extension reason\n",
      "==========================================\n",
      "[(1488, 1), (14, 1), (3839, 1)]\n",
      "[(0, 0.13396558193052982), (1, 0.86603441806947012)]\n",
      "person hate class\n",
      "==========================================\n",
      "[(802, 1), (1738, 1), (1527, 1), (120, 1), (2107, 1), (3839, 1)]\n",
      "[(0, 0.094336944276789111), (1, 0.90566305572321082)]\n",
      "paradigm region way study intro class\n",
      "==========================================\n",
      "[(288, 1), (3505, 1), (1954, 1), (435, 1), (3975, 1)]\n",
      "[(0, 0.90377439056216191), (1, 0.096225609437838144)]\n",
      "feedback process revision draft job\n",
      "==========================================\n",
      "[(2313, 1), (2348, 1), (3727, 1)]\n",
      "[(0, 0.87462977021745125), (1, 0.12537022978254872)]\n",
      "family problem set\n",
      "==========================================\n",
      "[(513, 1), (4072, 1), (1358, 1), (1076, 1), (410, 1), (2300, 1), (3839, 1)]\n",
      "[(0, 0.41038509943511459), (1, 0.58961490056488541)]\n",
      "professor frustrating course time size experience class\n",
      "==========================================\n",
      "[(3681, 1), (3051, 1), (1076, 1)]\n",
      "[(0, 0.23937840776875385), (1, 0.76062159223124626)]\n",
      "lecture bit time\n",
      "==========================================\n",
      "[(1488, 1), (3681, 1), (1102, 1), (3839, 1)]\n",
      "[(0, 0.10464654660479686), (1, 0.895353453395203)]\n",
      "person lecture zone class\n",
      "==========================================\n",
      "[(1088, 1), (4388, 1), (425, 1), (1196, 1), (1358, 2), (1661, 1), (1076, 1), (2540, 1), (4127, 1)]\n",
      "[(0, 0.92126840564362056), (1, 0.078731594356379497)]\n",
      "focus year priority teacher course enthusiasm time research student\n",
      "==========================================\n",
      "[(1300, 1), (1717, 1), (3839, 1)]\n",
      "[(0, 0.1255655566556364), (1, 0.87443444334436371)]\n",
      "discussion reading class\n",
      "==========================================\n",
      "[(2055, 1), (1717, 1), (3839, 1)]\n",
      "[(0, 0.16680071994919221), (1, 0.83319928005080779)]\n",
      "lot reading class\n",
      "==========================================\n",
      "[(3839, 1), (303, 1)]\n",
      "[(0, 0.48544377048902126), (1, 0.51455622951097879)]\n",
      "class writing\n",
      "==========================================\n",
      "[(303, 1), (2462, 1), (3839, 1)]\n",
      "[(0, 0.44300701671436693), (1, 0.55699298328563307)]\n",
      "writing style class\n",
      "==========================================\n",
      "[(1258, 2), (3839, 1)]\n",
      "[(0, 0.139127391638206), (1, 0.86087260836179402)]\n",
      "effort class\n",
      "==========================================\n",
      "[(1825, 1), (2339, 1), (2055, 1)]\n",
      "[(0, 0.86655676830945194), (1, 0.13344323169054814)]\n",
      "return homework lot\n",
      "==========================================\n",
      "[(3681, 1), (3903, 1), (3839, 1)]\n",
      "[(0, 0.13096334520257158), (1, 0.86903665479742842)]\n",
      "lecture background class\n",
      "==========================================\n",
      "[(1800, 1), (361, 1), (4017, 1), (120, 1), (2300, 1), (2462, 1), (3839, 1)]\n",
      "[(0, 0.2352270027561332), (1, 0.76477299724386683)]\n",
      "case taught undergraduate study experience style class\n",
      "==========================================\n",
      "[(185, 1), (772, 1), (1149, 1), (2055, 1)]\n",
      "[(0, 0.88877560798009392), (1, 0.11122439201990604)]\n",
      "room screw work lot\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "for bow in corpus[0:1200:60]:\n",
    "    print bow\n",
    "    print lda2.get_document_topics(bow)\n",
    "    print \" \".join([id2word[e[0]] for e in bow])\n",
    "    print \"==========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"sentences\" (or bag-of-words) which have a much greater probability of belonging to Topic 0 include:\n",
    "* class writer load work\n",
    "* class bit pass work\n",
    "* class cost intervention u'very benefit reform lot issue\n",
    "* class student success belief stand risk course question ability\n",
    "* class kink\n",
    "* way professor class discussion material\n",
    "* food professor person class\n",
    "* class section discussion student\n",
    "* time homework week\n",
    "\n",
    "The words in these sentences are more descriptive and relate to more creative, interactive, person-to-person aspects of a course. Specifically, \"writer\", \"intervention\", \"reform\", \"issue\", \"success\", \"belief\", \"stand\", \"risk\", \"question\", \"kink\", and \"discussion\" all imply rich and diverse elements of the course experience. As a Harvard student, I know that the word \"section\" also implies discussion and collaboration since sections for Harvard classes provide an opportunity outside of lecture to engage more closely with course material and consist of tight-knit groups.\n",
    "\n",
    "The \"sentences\" which have a much greater probability of belonging to Topic 1 include:\n",
    "* concept background\n",
    "* reading\n",
    "* thought-provoking discussion debate material staff teaching\n",
    "\n",
    "The function below transforms X-col (which consists of word-based \"sentences\" (bag-of-words or \"documents\")) using the vectorizer which is also a parameter.\n",
    "* time bit course u\"must\n",
    "* career course regret\n",
    "* education perspective lot debate history\n",
    "\n",
    "Words in these sentences that are not present in the previous cluster of setences and that stand out as implying more impersonal, logistical, or practical aspects of a course include \"concept\", \"background\", \"reading\", \"material\", \"time\", \"must\", \"education\", \"history\", and arguably \"staff\" (since \"staff\" is a somewhat impersonal way to refer to professors and teaching fellows). Although words like \"thought-provoking\", \"discussion\", \"debate\" and \"teaching\" do appear, it is worthwhile to note that the sentence/bag-of-words in which they appear still has a relatively high probability of belonging to Topic 0 (~35%).\n",
    "\n",
    "The \"sentences\" which have more equal probabilities of belonging to Topic 0 or 1 include:\n",
    "* time assignment philosophy night\n",
    "* course lot\n",
    "* chance education course lot style\n",
    "* student reason\n",
    "* way class overview study\n",
    "\n",
    "For sentences/bag-of-words with relatively equal probabilities of belonging to Topic 0 and Topic 1, we can observe both words implying more interactive, creative aspects (\"philosophy\", \"style\", \"student\", \"reason\") and words implying more logistical aspects (\"time\", \"assignment\", \"overview\", \"study\").\n",
    "\n",
    "From our analysis of the topic probabilities and bag-of-words above, there appears to be evidence to support our initial hypothesis that course reviews are split along two topics: Topic 0, which includes more interactive, qualitative, personable aspects of a course, and Topic 1, which includes more solitary, logistical aspects.\n",
    "\n",
    "Let us now continue with a sentiment analysis of the adjectives in the comments using Naive Bayes. We begin by extracting the adjectives as we did before with the nouns. As before, we create adjvocabtups, a list of tuples of with adjectives and index, and we create adjvocab, a dictionary mapping adjectives to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'awesome'],\n",
       "  [u'dense', u'occasional', u'terrible'],\n",
       "  [u'interesting', u'intensive', u'difficult'],\n",
       "  [u'interesting',\n",
       "   u'mandatory',\n",
       "   u'solid',\n",
       "   u'important',\n",
       "   u'interesting',\n",
       "   u'philosophical'],\n",
       "  [u'great'],\n",
       "  [u'concrete', u'greater', u'effective'],\n",
       "  [u'difficult', u'accurate'],\n",
       "  [u'interesting', u'great'],\n",
       "  [u'critical'],\n",
       "  [u'difficult', u'better'],\n",
       "  [u'subject', u'interesting'],\n",
       "  [u'difficult', u'unnecessary'],\n",
       "  [u'potential', u'interesting'],\n",
       "  [u'high', u'engaging'],\n",
       "  [u'fellow'],\n",
       "  [u'interesting', u'willing', u'interesting'],\n",
       "  [u'passionate', u'wide', u'fellow', u'similar']]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdatardd=sc.parallelize([ele[1] for ele in parsedcomments])\n",
    "nbdatardd.cache()\n",
    "nbdatardd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adjvocabtups = (nbdatardd.flatMap(lambda l: l).flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()).cache()\n",
    "adjvocab=adjvocabtups.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3139 unique adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "Xarraypre = nbdatardd.map(lambda l: \" \".join(list(itertools.chain.from_iterable(l))))\n",
    "Xarray = Xarraypre.collect()\n",
    "resparray = subdf.rdd.map(lambda r: r.Positive).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.fas.harvard.edu/~evals/  \n",
    "http://www.clipartpanda.com/categories/cool-question-marks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
